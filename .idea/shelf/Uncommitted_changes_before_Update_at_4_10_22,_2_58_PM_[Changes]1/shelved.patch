Index: examples/nodeproppred/mag/cluster_gcn.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from copy import copy\nimport argparse\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import ModuleList, Linear, ParameterDict, Parameter\nfrom torch_sparse import SparseTensor\nfrom torch_geometric.utils import to_undirected\nfrom torch_geometric.data import Data, ClusterData, ClusterLoader\nfrom torch_geometric.utils.hetero import group_hetero_graph\nfrom torch_geometric.nn import MessagePassing\n\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n\nfrom logger import Logger\n\nparser = argparse.ArgumentParser(description='OGBN-MAG (Cluster-GCN)')\nparser.add_argument('--device', type=int, default=0)\nparser.add_argument('--num_layers', type=int, default=2)\nparser.add_argument('--hidden_channels', type=int, default=64)\nparser.add_argument('--dropout', type=float, default=0.5)\nparser.add_argument('--lr', type=float, default=0.005)\nparser.add_argument('--epochs', type=int, default=30)\nparser.add_argument('--runs', type=int, default=10)\nargs = parser.parse_args()\nprint(args)\n\ndataset = PygNodePropPredDataset(name='ogbn-mag')\ndata = dataset[0]\nsplit_idx = dataset.get_idx_split()\nevaluator = Evaluator(name='ogbn-mag')\nlogger = Logger(args.runs, args)\n\n# We do not consider those attributes for now.\ndata.node_year_dict = None\ndata.edge_reltype_dict = None\n\nprint(data)\n\nedge_index_dict = data.edge_index_dict\n\n# We need to add reverse edges to the heterogeneous graph.\nr, c = edge_index_dict[('author', 'affiliated_with', 'institution')]\nedge_index_dict[('institution', 'to', 'author')] = torch.stack([c, r])\n\nr, c = edge_index_dict[('author', 'writes', 'paper')]\nedge_index_dict[('paper', 'to', 'author')] = torch.stack([c, r])\n\nr, c = edge_index_dict[('paper', 'has_topic', 'field_of_study')]\nedge_index_dict[('field_of_study', 'to', 'paper')] = torch.stack([c, r])\n\n# Convert to undirected paper <-> paper relation.\nedge_index = to_undirected(edge_index_dict[('paper', 'cites', 'paper')])\nedge_index_dict[('paper', 'cites', 'paper')] = edge_index\n\n# We convert the individual graphs into a single big one, so that sampling\n# neighbors does not need to care about different edge types.\n# This will return the following:\n# * `edge_index`: The new global edge connectivity.\n# * `edge_type`: The edge type for each edge.\n# * `node_type`: The node type for each node.\n# * `local_node_idx`: The original index for each node.\n# * `local2global`: A dictionary mapping original (local) node indices of\n#    type `key` to global ones.\n# `key2int`: A dictionary that maps original keys to their new canonical type.\nout = group_hetero_graph(data.edge_index_dict, data.num_nodes_dict)\nedge_index, edge_type, node_type, local_node_idx, local2global, key2int = out\n\nhomo_data = Data(edge_index=edge_index, edge_attr=edge_type,\n                 node_type=node_type, local_node_idx=local_node_idx,\n                 num_nodes=node_type.size(0))\n\nhomo_data.y = node_type.new_full((node_type.size(0), 1), -1)\nhomo_data.y[local2global['paper']] = data.y_dict['paper']\n\nhomo_data.train_mask = torch.zeros((node_type.size(0)), dtype=torch.bool)\nhomo_data.train_mask[local2global['paper'][split_idx['train']['paper']]] = True\n\nprint(homo_data)\n\ncluster_data = ClusterData(homo_data, num_parts=5000, recursive=True,\n                           save_dir=dataset.processed_dir)\ntrain_loader = ClusterLoader(cluster_data, batch_size=500, shuffle=True,\n                             num_workers=12)\n\n# Map informations to their canonical type.\nx_dict = {}\nfor key, x in data.x_dict.items():\n    x_dict[key2int[key]] = x\n\nnum_nodes_dict = {}\nfor key, N in data.num_nodes_dict.items():\n    num_nodes_dict[key2int[key]] = N\n\n\nclass RGCNConv(MessagePassing):\n    def __init__(self, in_channels, out_channels, num_node_types,\n                 num_edge_types):\n        super(RGCNConv, self).__init__(aggr='mean')\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_node_types = num_node_types\n        self.num_edge_types = num_edge_types\n\n        self.rel_lins = ModuleList([\n            Linear(in_channels, out_channels, bias=False)\n            for _ in range(num_edge_types)\n        ])\n\n        self.root_lins = ModuleList([\n            Linear(in_channels, out_channels, bias=True)\n            for _ in range(num_node_types)\n        ])\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for lin in self.rel_lins:\n            lin.reset_parameters()\n        for lin in self.root_lins:\n            lin.reset_parameters()\n\n    def forward(self, x, edge_index, edge_type, node_type):\n        out = x.new_zeros(x.size(0), self.out_channels)\n\n        for i in range(self.num_edge_types):\n            mask = edge_type == i\n            out.add_(self.propagate(edge_index[:, mask], x=x, edge_type=i))\n\n        for i in range(self.num_node_types):\n            mask = node_type == i\n            out[mask] += self.root_lins[i](x[mask])\n\n        return out\n\n    def message(self, x_j, edge_type: int):\n        return self.rel_lins[edge_type](x_j)\n\n\nclass RGCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout, num_nodes_dict, x_types, num_edge_types):\n        super(RGCN, self).__init__()\n\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        node_types = list(num_nodes_dict.keys())\n        num_node_types = len(node_types)\n\n        self.num_node_types = num_node_types\n        self.num_edge_types = num_edge_types\n\n        # Create embeddings for all node types that do not come with features.\n        self.emb_dict = ParameterDict({\n            f'{key}': Parameter(torch.Tensor(num_nodes_dict[key], in_channels))\n            for key in set(node_types).difference(set(x_types))\n        })\n\n        I, H, O = in_channels, hidden_channels, out_channels  # noqa\n\n        # Create `num_layers` many message passing layers.\n        self.convs = ModuleList()\n        self.convs.append(RGCNConv(I, H, num_node_types, num_edge_types))\n        for _ in range(num_layers - 2):\n            self.convs.append(RGCNConv(H, H, num_node_types, num_edge_types))\n        self.convs.append(RGCNConv(H, O, self.num_node_types, num_edge_types))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for emb in self.emb_dict.values():\n            torch.nn.init.xavier_uniform_(emb)\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def group_input(self, x_dict, node_type, local_node_idx):\n        # Create global node feature matrix.\n        h = torch.zeros((node_type.size(0), self.in_channels),\n                        device=node_type.device)\n\n        for key, x in x_dict.items():\n            mask = node_type == key\n            h[mask] = x[local_node_idx[mask]]\n\n        for key, emb in self.emb_dict.items():\n            mask = node_type == int(key)\n            h[mask] = emb[local_node_idx[mask]]\n\n        return h\n\n    def forward(self, x_dict, edge_index, edge_type, node_type,\n                local_node_idx):\n\n        x = self.group_input(x_dict, node_type, local_node_idx)\n\n        for i, conv in enumerate(self.convs):\n            x = conv(x, edge_index, edge_type, node_type)\n            if i != self.num_layers - 1:\n                x = F.relu(x)\n                x = F.dropout(x, p=0.5, training=self.training)\n\n        return x.log_softmax(dim=-1)\n\n    def inference(self, x_dict, edge_index_dict, key2int):\n        # We can perform full-batch inference on GPU.\n\n        device = list(x_dict.values())[0].device\n\n        x_dict = copy(x_dict)\n        for key, emb in self.emb_dict.items():\n            x_dict[int(key)] = emb\n\n        adj_t_dict = {}\n        for key, (row, col) in edge_index_dict.items():\n            adj_t_dict[key] = SparseTensor(row=col, col=row).to(device)\n\n        for i, conv in enumerate(self.convs):\n            out_dict = {}\n\n            for j, x in x_dict.items():\n                out_dict[j] = conv.root_lins[j](x)\n\n            for keys, adj_t in adj_t_dict.items():\n                src_key, target_key = keys[0], keys[-1]\n                out = out_dict[key2int[target_key]]\n                tmp = adj_t.matmul(x_dict[key2int[src_key]], reduce='mean')\n                out.add_(conv.rel_lins[key2int[keys]](tmp))\n\n            if i != self.num_layers - 1:\n                for j in range(self.num_node_types):\n                    F.relu_(out_dict[j])\n\n            x_dict = out_dict\n\n        return x_dict\n\n\ndevice = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n\nmodel = RGCN(128, args.hidden_channels, dataset.num_classes, args.num_layers,\n             args.dropout, num_nodes_dict, list(x_dict.keys()),\n             len(edge_index_dict.keys())).to(device)\n\nx_dict = {k: v.to(device) for k, v in x_dict.items()}\n\n\ndef train(epoch):\n    model.train()\n\n    pbar = tqdm(total=node_type.size(0))\n    pbar.set_description(f'Epoch {epoch:02d}')\n\n    total_loss = total_examples = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(x_dict, data.edge_index, data.edge_attr, data.node_type,\n                    data.local_node_idx)\n        out = out[data.train_mask]\n        y = data.y[data.train_mask].squeeze()\n        loss = F.nll_loss(out, y)\n        loss.backward()\n        optimizer.step()\n\n        num_examples = data.train_mask.sum().item()\n        total_loss += loss.item() * num_examples\n        total_examples += num_examples\n        pbar.update(data.node_type.size(0))\n\n    pbar.close()\n\n    return total_loss / total_examples\n\n\n@torch.no_grad()\ndef test():\n    model.eval()\n\n    out = model.inference(x_dict, edge_index_dict, key2int)\n    out = out[key2int['paper']]\n\n    y_pred = out.argmax(dim=-1, keepdim=True).cpu()\n    y_true = data.y_dict['paper']\n\n    train_acc = evaluator.eval({\n        'y_true': y_true[split_idx['train']['paper']],\n        'y_pred': y_pred[split_idx['train']['paper']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': y_true[split_idx['valid']['paper']],\n        'y_pred': y_pred[split_idx['valid']['paper']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': y_true[split_idx['test']['paper']],\n        'y_pred': y_pred[split_idx['test']['paper']],\n    })['acc']\n\n    return train_acc, valid_acc, test_acc\n\n\ntest()  # Test if inference on GPU succeeds.\nfor run in range(args.runs):\n    model.reset_parameters()\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n    for epoch in range(1, 1 + args.epochs):\n        loss = train(epoch)\n        torch.cuda.empty_cache()\n        result = test()\n        logger.add_result(run, result)\n        train_acc, valid_acc, test_acc = result\n        print(f'Run: {run + 1:02d}, '\n              f'Epoch: {epoch:02d}, '\n              f'Loss: {loss:.4f}, '\n              f'Train: {100 * train_acc:.2f}%, '\n              f'Valid: {100 * valid_acc:.2f}%, '\n              f'Test: {100 * test_acc:.2f}%')\n    logger.print_statistics(run)\nlogger.print_statistics()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/examples/nodeproppred/mag/cluster_gcn.py b/examples/nodeproppred/mag/cluster_gcn.py
--- a/examples/nodeproppred/mag/cluster_gcn.py	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/examples/nodeproppred/mag/cluster_gcn.py	(date 1649189007709)
@@ -41,18 +41,22 @@
 edge_index_dict = data.edge_index_dict
 
 # We need to add reverse edges to the heterogeneous graph.
-r, c = edge_index_dict[('author', 'affiliated_with', 'institution')]
-edge_index_dict[('institution', 'to', 'author')] = torch.stack([c, r])
+if ('author', 'affiliated_with', 'institution') in edge_index_dict.keys():
+    r, c = edge_index_dict[('author', 'affiliated_with', 'institution')]
+    edge_index_dict[('institution', 'to', 'author')] = torch.stack([c, r])
 
-r, c = edge_index_dict[('author', 'writes', 'paper')]
-edge_index_dict[('paper', 'to', 'author')] = torch.stack([c, r])
+if ('author', 'writes', 'paper') in edge_index_dict.keys():
+    r, c = edge_index_dict[('author', 'writes', 'paper')]
+    edge_index_dict[('paper', 'to', 'author')] = torch.stack([c, r])
 
-r, c = edge_index_dict[('paper', 'has_topic', 'field_of_study')]
-edge_index_dict[('field_of_study', 'to', 'paper')] = torch.stack([c, r])
+if ('paper', 'has_topic', 'field_of_study') in edge_index_dict.keys():
+    r, c = edge_index_dict[('paper', 'has_topic', 'field_of_study')]
+    edge_index_dict[('field_of_study', 'to', 'paper')] = torch.stack([c, r])
 
 # Convert to undirected paper <-> paper relation.
-edge_index = to_undirected(edge_index_dict[('paper', 'cites', 'paper')])
-edge_index_dict[('paper', 'cites', 'paper')] = edge_index
+if ('paper', 'cites', 'paper') in edge_index_dict.keys():
+    edge_index = to_undirected(edge_index_dict[('paper', 'cites', 'paper')])
+    edge_index_dict[('paper', 'cites', 'paper')] = edge_index
 
 # We convert the individual graphs into a single big one, so that sampling
 # neighbors does not need to care about different edge types.
@@ -85,8 +89,14 @@
                              num_workers=12)
 
 # Map informations to their canonical type.
+#######################intialize random features ###############################
+feat = torch.Tensor(data.num_nodes_dict['paper'], 128)
+torch.nn.init.xavier_uniform_(feat)
+feat_dic = {'paper':feat}
+################################################################
 x_dict = {}
-for key, x in data.x_dict.items():
+# for key, x in data.x_dict.items():
+for key, x in feat_dic.items():
     x_dict[key2int[key]] = x
 
 num_nodes_dict = {}
Index: .idea/ogb.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/ogb.iml b/.idea/ogb.iml
new file mode 100755
--- /dev/null	(date 1648251732806)
+++ b/.idea/ogb.iml	(date 1648251732806)
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="PYTHON_MODULE" version="4">
+  <component name="NewModuleRootManager">
+    <content url="file://$MODULE_DIR$" />
+    <orderEntry type="jdk" jdkName="Python 3.9 (ogb)" jdkType="Python SDK" />
+    <orderEntry type="sourceFolder" forTests="false" />
+  </component>
+</module>
\ No newline at end of file
Index: .idea/inspectionProfiles/profiles_settings.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
new file mode 100755
--- /dev/null	(date 1648250062172)
+++ b/.idea/inspectionProfiles/profiles_settings.xml	(date 1648250062172)
@@ -0,0 +1,6 @@
+<component name="InspectionProjectProfileManager">
+  <settings>
+    <option name="USE_PROJECT_PROFILE" value="false" />
+    <version value="1.0" />
+  </settings>
+</component>
\ No newline at end of file
Index: .idea/vcs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
new file mode 100755
--- /dev/null	(date 1648250062177)
+++ b/.idea/vcs.xml	(date 1648250062177)
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
Index: examples/nodeproppred/mag/rgcn.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import copy\nimport argparse\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Parameter, ModuleDict, ModuleList, Linear, ParameterDict\nfrom torch_sparse import SparseTensor\n\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n\nfrom logger import Logger\n\n\nclass RGCNConv(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, node_types, edge_types):\n        super(RGCNConv, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        # `ModuleDict` does not allow tuples :(\n        self.rel_lins = ModuleDict({\n            f'{key[0]}_{key[1]}_{key[2]}': Linear(in_channels, out_channels,\n                                                  bias=False)\n            for key in edge_types\n        })\n\n        self.root_lins = ModuleDict({\n            key: Linear(in_channels, out_channels, bias=True)\n            for key in node_types\n        })\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for lin in self.rel_lins.values():\n            lin.reset_parameters()\n        for lin in self.root_lins.values():\n            lin.reset_parameters()\n\n    def forward(self, x_dict, adj_t_dict):\n        out_dict = {}\n        for key, x in x_dict.items():\n            out_dict[key] = self.root_lins[key](x)\n\n        for key, adj_t in adj_t_dict.items():\n            key_str = f'{key[0]}_{key[1]}_{key[2]}'\n            x = x_dict[key[0]]\n            out = self.rel_lins[key_str](adj_t.matmul(x, reduce='mean'))\n            out_dict[key[2]].add_(out)\n\n        return out_dict\n\n\nclass RGCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout, num_nodes_dict, x_types, edge_types):\n        super(RGCN, self).__init__()\n\n        node_types = list(num_nodes_dict.keys())\n\n        self.embs = ParameterDict({\n            key: Parameter(torch.Tensor(num_nodes_dict[key], in_channels))\n            for key in set(node_types).difference(set(x_types))\n        })\n\n        self.convs = ModuleList()\n        self.convs.append(\n            RGCNConv(in_channels, hidden_channels, node_types, edge_types))\n        for _ in range(num_layers - 2):\n            self.convs.append(\n                RGCNConv(hidden_channels, hidden_channels, node_types,\n                         edge_types))\n        self.convs.append(\n            RGCNConv(hidden_channels, out_channels, node_types, edge_types))\n\n        self.dropout = dropout\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for emb in self.embs.values():\n            torch.nn.init.xavier_uniform_(emb)\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x_dict, adj_t_dict):\n        x_dict = copy.copy(x_dict)\n        for key, emb in self.embs.items():\n            x_dict[key] = emb\n\n        for conv in self.convs[:-1]:\n            x_dict = conv(x_dict, adj_t_dict)\n            for key, x in x_dict.items():\n                x_dict[key] = F.relu(x)\n                x_dict[key] = F.dropout(x, p=self.dropout,\n                                        training=self.training)\n        return self.convs[-1](x_dict, adj_t_dict)\n\n\ndef train(model, x_dict, adj_t_dict, y_true, train_idx, optimizer):\n    model.train()\n\n    optimizer.zero_grad()\n    out = model(x_dict, adj_t_dict)['paper'].log_softmax(dim=-1)\n    loss = F.nll_loss(out[train_idx], y_true[train_idx].squeeze())\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n\n\n@torch.no_grad()\ndef test(model, x_dict, adj_t_dict, y_true, split_idx, evaluator):\n    model.eval()\n\n    out = model(x_dict, adj_t_dict)['paper']\n    y_pred = out.argmax(dim=-1, keepdim=True)\n\n    train_acc = evaluator.eval({\n        'y_true': y_true[split_idx['train']['paper']],\n        'y_pred': y_pred[split_idx['train']['paper']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': y_true[split_idx['valid']['paper']],\n        'y_pred': y_pred[split_idx['valid']['paper']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': y_true[split_idx['test']['paper']],\n        'y_pred': y_pred[split_idx['test']['paper']],\n    })['acc']\n\n    return train_acc, valid_acc, test_acc\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='OGBN-MAG (Full-Batch)')\n    parser.add_argument('--device', type=int, default=0)\n    parser.add_argument('--log_steps', type=int, default=1)\n    parser.add_argument('--num_layers', type=int, default=2)\n    parser.add_argument('--hidden_channels', type=int, default=64)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--lr', type=float, default=0.01)\n    parser.add_argument('--epochs', type=int, default=50)\n    parser.add_argument('--runs', type=int, default=10)\n    args = parser.parse_args()\n    print(args)\n\n    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n\n    dataset = PygNodePropPredDataset(name='ogbn-mag')\n    split_idx = dataset.get_idx_split()\n    data = dataset[0]\n\n    # We do not consider those attributes for now.\n    data.node_year_dict = None\n    data.edge_reltype_dict = None\n\n    print(data)\n\n    # Convert to new transposed `SparseTensor` format and add reverse edges.\n    data.adj_t_dict = {}\n    for keys, (row, col) in data.edge_index_dict.items():\n        sizes = (data.num_nodes_dict[keys[0]], data.num_nodes_dict[keys[2]])\n        adj = SparseTensor(row=row, col=col, sparse_sizes=sizes)\n        # adj = SparseTensor(row=row, col=col)[:sizes[0], :sizes[1]] # TEST\n        if keys[0] != keys[2]:\n            data.adj_t_dict[keys] = adj.t()\n            data.adj_t_dict[(keys[2], 'to', keys[0])] = adj\n        else:\n            data.adj_t_dict[keys] = adj.to_symmetric()\n    data.edge_index_dict = None\n\n    x_types = list(data.x_dict.keys())\n    edge_types = list(data.adj_t_dict.keys())\n\n    model = RGCN(data.x_dict['paper'].size(-1), args.hidden_channels,\n                 dataset.num_classes, args.num_layers, args.dropout,\n                 data.num_nodes_dict, x_types, edge_types)\n\n    data = data.to(device)\n    model = model.to(device)\n    train_idx = split_idx['train']['paper'].to(device)\n\n    evaluator = Evaluator(name='ogbn-mag')\n    logger = Logger(args.runs, args)\n\n    for run in range(args.runs):\n        model.reset_parameters()\n        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n        for epoch in range(1, 1 + args.epochs):\n            loss = train(model, data.x_dict, data.adj_t_dict,\n                         data.y_dict['paper'], train_idx, optimizer)\n            result = test(model, data.x_dict, data.adj_t_dict,\n                          data.y_dict['paper'], split_idx, evaluator)\n            logger.add_result(run, result)\n\n            if epoch % args.log_steps == 0:\n                train_acc, valid_acc, test_acc = result\n                print(f'Run: {run + 1:02d}, '\n                      f'Epoch: {epoch:02d}, '\n                      f'Loss: {loss:.4f}, '\n                      f'Train: {100 * train_acc:.2f}%, '\n                      f'Valid: {100 * valid_acc:.2f}% '\n                      f'Test: {100 * test_acc:.2f}%')\n\n        logger.print_statistics(run)\n    logger.print_statistics()\n\n\nif __name__ == \"__main__\":\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/examples/nodeproppred/mag/rgcn.py b/examples/nodeproppred/mag/rgcn.py
--- a/examples/nodeproppred/mag/rgcn.py	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/examples/nodeproppred/mag/rgcn.py	(date 1649353569211)
@@ -1,12 +1,21 @@
-import copy
 import argparse
+import copy
+# !conda install pyg -c pygmport argparse# !conda install pyg -c pyg
+import datetime
 
+import pandas
 import torch
 import torch.nn.functional as F
 from torch.nn import Parameter, ModuleDict, ModuleList, Linear, ParameterDict
+## conda install pyg -c pyg
 from torch_sparse import SparseTensor
-
+#pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html
+# pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html
+# pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html
+# pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html
+# pip install torch-geometric
 from ogb.nodeproppred import PygNodePropPredDataset, Evaluator
+from resource import *
 
 from logger import Logger
 
@@ -18,14 +27,13 @@
         self.in_channels = in_channels
         self.out_channels = out_channels
 
-        # `ModuleDict` does not allow tuples :(
+        # `ModuleDict` does not allow tuples :( ## create linear layer for each predicate type i.e writes, affaliated with
         self.rel_lins = ModuleDict({
-            f'{key[0]}_{key[1]}_{key[2]}': Linear(in_channels, out_channels,
-                                                  bias=False)
+            f'{key[0]}_{key[1]}_{key[2]}': Linear(in_channels, out_channels,bias=False)
             for key in edge_types
         })
 
-        self.root_lins = ModuleDict({
+        self.root_lins = ModuleDict({ ## create linear layer for each node type (distinct veriex i.e author,paper,...)
             key: Linear(in_channels, out_channels, bias=True)
             for key in node_types
         })
@@ -38,7 +46,7 @@
         for lin in self.root_lins.values():
             lin.reset_parameters()
 
-    def forward(self, x_dict, adj_t_dict):
+    def forward(self, x_dict, adj_t_dict): ## aggregate updates
         out_dict = {}
         for key, x in x_dict.items():
             out_dict[key] = self.root_lins[key](x)
@@ -46,7 +54,7 @@
         for key, adj_t in adj_t_dict.items():
             key_str = f'{key[0]}_{key[1]}_{key[2]}'
             x = x_dict[key[0]]
-            out = self.rel_lins[key_str](adj_t.matmul(x, reduce='mean'))
+            out = self.rel_lins[key_str](adj_t.matmul(x, reduce='max'))
             out_dict[key[2]].add_(out)
 
         return out_dict
@@ -58,46 +66,55 @@
         super(RGCN, self).__init__()
 
         node_types = list(num_nodes_dict.keys())
-
-        self.embs = ParameterDict({
-            key: Parameter(torch.Tensor(num_nodes_dict[key], in_channels))
+        self.x_dict=None
+        self.embs = ParameterDict({ ## set node embedding features for all types except paper
+            key: Parameter(torch.Tensor(num_nodes_dict[key], in_channels)) ## vertixcount*embedding size
             for key in set(node_types).difference(set(x_types))
         })
 
         self.convs = ModuleList()
         self.convs.append(
-            RGCNConv(in_channels, hidden_channels, node_types, edge_types))
-        for _ in range(num_layers - 2):
+            RGCNConv(in_channels, hidden_channels, node_types, edge_types)) ## Start layer
+        for _ in range(num_layers - 2):## hidden Layers
             self.convs.append(
-                RGCNConv(hidden_channels, hidden_channels, node_types,
-                         edge_types))
-        self.convs.append(
-            RGCNConv(hidden_channels, out_channels, node_types, edge_types))
+                RGCNConv(hidden_channels, hidden_channels, node_types,edge_types))
+        self.convs.append(RGCNConv(hidden_channels, out_channels, node_types, edge_types)) ## output layer
 
         self.dropout = dropout
-
         self.reset_parameters()
 
     def reset_parameters(self):
         for emb in self.embs.values():
-            torch.nn.init.xavier_uniform_(emb)
+            torch.nn.init.xavier_uniform_(emb) ## intialize embeddinga with Xavier uniform dist
         for conv in self.convs:
             conv.reset_parameters()
 
+    # def forward(self, x_dict, adj_t_dict):
+    #     if self.x_dict==None:
+    #         self.x_dict = copy.copy(x_dict) ## copy x_dict features
+    #         for key, emb in self.embs.items():
+    #             self.x_dict[key] = emb
+    #
+    #     for conv in self.convs[:-1]:
+    #         self.x_dict = conv(self.x_dict, adj_t_dict) ## update features from by convolution layer forward (mean)
+    #         x_dict = copy.copy(self.x_dict)
+    #         for key, x in self.x_dict.items():
+    #             x_dict[key] = F.relu(x) ## relu
+    #             x_dict[key] = F.dropout(x, p=self.dropout,training=self.training) ## dropout some updated features
+    #     return self.convs[-1](x_dict, adj_t_dict)
+
     def forward(self, x_dict, adj_t_dict):
-        x_dict = copy.copy(x_dict)
+        x_dict = copy.copy(x_dict) ## copy x_dict features
         for key, emb in self.embs.items():
             x_dict[key] = emb
 
         for conv in self.convs[:-1]:
-            x_dict = conv(x_dict, adj_t_dict)
+            x_dict = conv(x_dict, adj_t_dict) ## update features from by convolution layer forward (mean)
             for key, x in x_dict.items():
-                x_dict[key] = F.relu(x)
-                x_dict[key] = F.dropout(x, p=self.dropout,
-                                        training=self.training)
+                x_dict[key] = F.relu(x) ## relu
+                x_dict[key] = F.dropout(x, p=self.dropout,training=self.training) ## dropout some updated features
         return self.convs[-1](x_dict, adj_t_dict)
 
-
 def train(model, x_dict, adj_t_dict, y_true, train_idx, optimizer):
     model.train()
 
@@ -133,7 +150,9 @@
     return train_acc, valid_acc, test_acc
 
 
+
 def main():
+    print(getrusage(RUSAGE_SELF))
     parser = argparse.ArgumentParser(description='OGBN-MAG (Full-Batch)')
     parser.add_argument('--device', type=int, default=0)
     parser.add_argument('--log_steps', type=int, default=1)
@@ -142,17 +161,27 @@
     parser.add_argument('--dropout', type=float, default=0.5)
     parser.add_argument('--lr', type=float, default=0.01)
     parser.add_argument('--epochs', type=int, default=50)
-    parser.add_argument('--runs', type=int, default=10)
+    parser.add_argument('--runs', type=int, default=3)
+    parser.add_argument('--loadTrainedModel', type=int, default=1)
     args = parser.parse_args()
     print(args)
 
     device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'
     device = torch.device(device)
-
+    start_t = datetime.datetime.now()
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-QM0')
+    #dataset = PygNodePropPredDataset(name='ogbn-mag-QM1')
     dataset = PygNodePropPredDataset(name='ogbn-mag')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-QM2')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-QM3')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-QM4')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-FM')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag_QM_paper_venue')
+
     split_idx = dataset.get_idx_split()
     data = dataset[0]
-
+    end_t = datetime.datetime.now()
+    print("dataset init time=", end_t - start_t, " sec.")
     # We do not consider those attributes for now.
     data.node_year_dict = None
     data.edge_reltype_dict = None
@@ -165,48 +194,85 @@
         sizes = (data.num_nodes_dict[keys[0]], data.num_nodes_dict[keys[2]])
         adj = SparseTensor(row=row, col=col, sparse_sizes=sizes)
         # adj = SparseTensor(row=row, col=col)[:sizes[0], :sizes[1]] # TEST
-        if keys[0] != keys[2]:
+        if keys[0] != keys[2]: ## subject and object are diffrent
             data.adj_t_dict[keys] = adj.t()
             data.adj_t_dict[(keys[2], 'to', keys[0])] = adj
         else:
             data.adj_t_dict[keys] = adj.to_symmetric()
     data.edge_index_dict = None
 
-    x_types = list(data.x_dict.keys())
+
     edge_types = list(data.adj_t_dict.keys())
-
-    model = RGCN(data.x_dict['paper'].size(-1), args.hidden_channels,
+    start_t = datetime.datetime.now()
+    # x_types = list(data.x_dict.keys())
+    x_types='paper'
+    ## data.x_dict['paper'] features of papers
+    ##RGCN(in_channels, hidden_channels, out_channels, num_layers,    dropout, num_nodes_dict, x_types, edge_types)
+    ##data.x_dict['paper'].size(-1)=128= embedding vector size
+    # torch.nn.init.xavier_uniform_(emb)
+    ############init papers with random embeddings #######################
+    # len(data.x_dict['paper'][0])
+    feat = torch.Tensor(data.num_nodes_dict['paper'], 128)
+    torch.nn.init.xavier_uniform_(feat)
+    feat_dic = {'paper':feat}
+    #####################################
+    # data.x_dict['paper'].size(-1)
+    model = RGCN(feat.size(-1) , args.hidden_channels,
                  dataset.num_classes, args.num_layers, args.dropout,
                  data.num_nodes_dict, x_types, edge_types)
-
-    data = data.to(device)
-    model = model.to(device)
     train_idx = split_idx['train']['paper'].to(device)
-
+    end_t = datetime.datetime.now()
     evaluator = Evaluator(name='ogbn-mag')
     logger = Logger(args.runs, args)
+    end_t = datetime.datetime.now()
+    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
+    if args.loadTrainedModel==1:
+        model.load_state_dict(torch.load("ogbn-mag-FM-RGCN.model"))
+        model.eval()
+        out = model(feat_dic, data.adj_t_dict)['paper']
+        y_pred = out.argmax(dim=-1, keepdim=True)
+        out_lst=torch.flatten(data.y_dict['paper']).tolist()
+        pred_lst = torch.flatten(y_pred).tolist()
+        out_df = pandas.DataFrame({"y_pred":pred_lst,"y_true":out_lst})
+        # print(y_pred, data.y_dict['paper'])
+        # print(out_df)
+        out_df.to_csv("RGCN_mag_output.csv",index=None)
+        # train_acc, valid_acc, test_acc = test(model, feat_dic, data.adj_t_dict,data.y_dict['paper'], split_idx, evaluator)
+        # print(f'Run: {-1 + 1:02d}, '
+        #       f'Train: {100 * train_acc:.2f}%, '
+        #       f'Valid: {100 * valid_acc:.2f}% '
+        #       f'Test: {100 * test_acc:.2f}%')
+    else:
+        data = data.to(device)
+        model = model.to(device)
 
-    for run in range(args.runs):
-        model.reset_parameters()
-        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
-        for epoch in range(1, 1 + args.epochs):
-            loss = train(model, data.x_dict, data.adj_t_dict,
-                         data.y_dict['paper'], train_idx, optimizer)
-            result = test(model, data.x_dict, data.adj_t_dict,
-                          data.y_dict['paper'], split_idx, evaluator)
-            logger.add_result(run, result)
+        print("model init time CPU=", end_t - start_t, " sec.")
+        for run in range(args.runs):
+            start_t = datetime.datetime.now()
+            model.reset_parameters()
+
+            for epoch in range(1, 1 + args.epochs):
+                loss = train(model, feat_dic, data.adj_t_dict,
+                             data.y_dict['paper'], train_idx, optimizer)
+                result = test(model, feat_dic, data.adj_t_dict,
+                              data.y_dict['paper'], split_idx, evaluator)
+                logger.add_result(run, result)
 
-            if epoch % args.log_steps == 0:
-                train_acc, valid_acc, test_acc = result
-                print(f'Run: {run + 1:02d}, '
-                      f'Epoch: {epoch:02d}, '
-                      f'Loss: {loss:.4f}, '
-                      f'Train: {100 * train_acc:.2f}%, '
-                      f'Valid: {100 * valid_acc:.2f}% '
-                      f'Test: {100 * test_acc:.2f}%')
-
-        logger.print_statistics(run)
-    logger.print_statistics()
+                if epoch % args.log_steps == 0:
+                    train_acc, valid_acc, test_acc = result
+                    print(f'Run: {run + 1:02d}, '
+                          f'Epoch: {epoch:02d}, '
+                          f'Loss: {loss:.4f}, '
+                          f'Train: {100 * train_acc:.2f}%, '
+                          f'Valid: {100 * valid_acc:.2f}% '
+                          f'Test: {100 * test_acc:.2f}%')
+            end_t = datetime.datetime.now()
+            logger.print_statistics(run)
+            print("model run ",run," train time CPU=", end_t - start_t, " sec.")
+            print(getrusage(RUSAGE_SELF))
+        logger.print_statistics()
+        end_t = datetime.datetime.now()
+        torch.save(model.state_dict(), "ogbn-mag-FM-RGCN.model")
 
 
 if __name__ == "__main__":
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
new file mode 100755
--- /dev/null	(date 1648251732812)
+++ b/.idea/misc.xml	(date 1648251732812)
@@ -0,0 +1,4 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.9 (ogb)" project-jdk-type="Python SDK" />
+</project>
\ No newline at end of file
Index: examples/nodeproppred/mag/gnn.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import argparse\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch_geometric.data import Data\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv, SAGEConv\n\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n\nfrom logger import Logger\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout):\n        super(GCN, self).__init__()\n\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(\n            GCNConv(in_channels, hidden_channels, normalize=False))\n        for _ in range(num_layers - 2):\n            self.convs.append(\n                GCNConv(hidden_channels, hidden_channels, normalize=False))\n        self.convs.append(\n            GCNConv(hidden_channels, out_channels, normalize=False))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x, adj_t):\n        for i, conv in enumerate(self.convs[:-1]):\n            x = conv(x, adj_t)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, adj_t)\n        return x.log_softmax(dim=-1)\n\n\nclass SAGE(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout):\n        super(SAGE, self).__init__()\n\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(SAGEConv(in_channels, hidden_channels))\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n        self.convs.append(SAGEConv(hidden_channels, out_channels))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x, adj_t):\n        for i, conv in enumerate(self.convs[:-1]):\n            x = conv(x, adj_t)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, adj_t)\n        return x.log_softmax(dim=-1)\n\n\ndef train(model, data, train_idx, optimizer):\n    model.train()\n\n    optimizer.zero_grad()\n    out = model(data.x, data.adj_t)[train_idx]\n    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n\n\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    model.eval()\n\n    out = model(data.x, data.adj_t)\n    y_pred = out.argmax(dim=-1, keepdim=True)\n\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']['paper']],\n        'y_pred': y_pred[split_idx['train']['paper']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']['paper']],\n        'y_pred': y_pred[split_idx['valid']['paper']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']['paper']],\n        'y_pred': y_pred[split_idx['test']['paper']],\n    })['acc']\n\n    return train_acc, valid_acc, test_acc\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='OGBN-MAG (GNN)')\n    parser.add_argument('--device', type=int, default=0)\n    parser.add_argument('--log_steps', type=int, default=1)\n    parser.add_argument('--use_sage', action='store_true')\n    parser.add_argument('--num_layers', type=int, default=2)\n    parser.add_argument('--hidden_channels', type=int, default=256)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--lr', type=float, default=0.01)\n    parser.add_argument('--epochs', type=int, default=100)\n    parser.add_argument('--runs', type=int, default=10)\n    args = parser.parse_args()\n    print(args)\n\n    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n\n    dataset = PygNodePropPredDataset(name='ogbn-mag')\n    rel_data = dataset[0]\n\n    # We are only interested in paper <-> paper relations.\n    data = Data(\n        x=rel_data.x_dict['paper'],\n        edge_index=rel_data.edge_index_dict[('paper', 'cites', 'paper')],\n        y=rel_data.y_dict['paper'])\n\n    data = T.ToSparseTensor()(data)\n    data.adj_t = data.adj_t.to_symmetric()\n\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx['train']['paper'].to(device)\n\n    if args.use_sage:\n        model = SAGE(data.num_features, args.hidden_channels,\n                     dataset.num_classes, args.num_layers,\n                     args.dropout).to(device)\n    else:\n        model = GCN(data.num_features, args.hidden_channels,\n                    dataset.num_classes, args.num_layers,\n                    args.dropout).to(device)\n\n        # Pre-compute GCN normalization.\n        adj_t = data.adj_t.set_diag()\n        deg = adj_t.sum(dim=1).to(torch.float)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n        data.adj_t = adj_t\n\n    data = data.to(device)\n\n    evaluator = Evaluator(name='ogbn-mag')\n    logger = Logger(args.runs, args)\n\n    for run in range(args.runs):\n        model.reset_parameters()\n        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n        for epoch in range(1, 1 + args.epochs):\n            loss = train(model, data, train_idx, optimizer)\n            result = test(model, data, split_idx, evaluator)\n            logger.add_result(run, result)\n\n            if epoch % args.log_steps == 0:\n                train_acc, valid_acc, test_acc = result\n                print(f'Run: {run + 1:02d}, '\n                      f'Epoch: {epoch:02d}, '\n                      f'Loss: {loss:.4f}, '\n                      f'Train: {100 * train_acc:.2f}%, '\n                      f'Valid: {100 * valid_acc:.2f}% '\n                      f'Test: {100 * test_acc:.2f}%')\n\n        logger.print_statistics(run)\n    logger.print_statistics()\n\n\nif __name__ == \"__main__\":\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/examples/nodeproppred/mag/gnn.py b/examples/nodeproppred/mag/gnn.py
--- a/examples/nodeproppred/mag/gnn.py	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/examples/nodeproppred/mag/gnn.py	(date 1649353601040)
@@ -1,8 +1,9 @@
 import argparse
 
+import pandas
 import torch
 import torch.nn.functional as F
-
+import datetime
 from torch_geometric.data import Data
 import torch_geometric.transforms as T
 from torch_geometric.nn import GCNConv, SAGEConv
@@ -10,7 +11,7 @@
 from ogb.nodeproppred import PygNodePropPredDataset, Evaluator
 
 from logger import Logger
-
+from resource import *
 
 class GCN(torch.nn.Module):
     def __init__(self, in_channels, hidden_channels, out_channels, num_layers,
@@ -103,6 +104,7 @@
 
 
 def main():
+    print(getrusage(RUSAGE_SELF))
     parser = argparse.ArgumentParser(description='OGBN-MAG (GNN)')
     parser.add_argument('--device', type=int, default=0)
     parser.add_argument('--log_steps', type=int, default=1)
@@ -112,28 +114,40 @@
     parser.add_argument('--dropout', type=float, default=0.5)
     parser.add_argument('--lr', type=float, default=0.01)
     parser.add_argument('--epochs', type=int, default=100)
-    parser.add_argument('--runs', type=int, default=10)
+    parser.add_argument('--runs', type=int, default=3)
+    parser.add_argument('--loadTrainedModel', type=int, default=1)
     args = parser.parse_args()
     print(args)
 
     device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'
     device = torch.device(device)
-
+    start_t = datetime.datetime.now()
     dataset = PygNodePropPredDataset(name='ogbn-mag')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-QM3')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-QM2')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-QM4')
+    # dataset = PygNodePropPredDataset(name='ogbn-mag-QM1')
     rel_data = dataset[0]
 
     # We are only interested in paper <-> paper relations.
+    feat = torch.Tensor(rel_data.num_nodes_dict['paper'], 128)
+    torch.nn.init.xavier_uniform_(feat)
+    # feat_dic = {'paper': feat}
+
     data = Data(
         x=rel_data.x_dict['paper'],
+        # x=feat,
         edge_index=rel_data.edge_index_dict[('paper', 'cites', 'paper')],
         y=rel_data.y_dict['paper'])
 
     data = T.ToSparseTensor()(data)
     data.adj_t = data.adj_t.to_symmetric()
-
     split_idx = dataset.get_idx_split()
     train_idx = split_idx['train']['paper'].to(device)
+    end_t = datetime.datetime.now()
+    print("dataset init time=", end_t - start_t, " sec.")
 
+    start_t = datetime.datetime.now()
     if args.use_sage:
         model = SAGE(data.num_features, args.hidden_channels,
                      dataset.num_classes, args.num_layers,
@@ -155,27 +169,44 @@
 
     evaluator = Evaluator(name='ogbn-mag')
     logger = Logger(args.runs, args)
-
-    for run in range(args.runs):
-        model.reset_parameters()
-        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
-        for epoch in range(1, 1 + args.epochs):
-            loss = train(model, data, train_idx, optimizer)
-            result = test(model, data, split_idx, evaluator)
-            logger.add_result(run, result)
+    end_t = datetime.datetime.now()
+    print("model init time CPU=", end_t - start_t, " sec.")
+    if args.loadTrainedModel==1:
+        model.load_state_dict(torch.load("ogbn-mag-FM-GCN.model"))
+        model.eval()
+        out = model(data.x, data.adj_t)
+        y_pred = out.argmax(dim=-1, keepdim=True)
+        out_lst=torch.flatten(data.y).tolist()
+        pred_lst = torch.flatten(y_pred).tolist()
+        out_df = pandas.DataFrame({"y_pred":pred_lst,"y_true":out_lst})
+        # print(y_pred, data.y_dict['paper'])
+        # print(out_df)
+        out_df.to_csv("GCN_mag_output.csv",index=None)
+    else:
+        for run in range(args.runs):
+            start_t = datetime.datetime.now()
+            model.reset_parameters()
+            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
+            for epoch in range(1, 1 + args.epochs):
+                loss = train(model, data, train_idx, optimizer)
+                result = test(model, data, split_idx, evaluator)
+                logger.add_result(run, result)
 
-            if epoch % args.log_steps == 0:
-                train_acc, valid_acc, test_acc = result
-                print(f'Run: {run + 1:02d}, '
-                      f'Epoch: {epoch:02d}, '
-                      f'Loss: {loss:.4f}, '
-                      f'Train: {100 * train_acc:.2f}%, '
-                      f'Valid: {100 * valid_acc:.2f}% '
-                      f'Test: {100 * test_acc:.2f}%')
+                if epoch % args.log_steps == 0:
+                    train_acc, valid_acc, test_acc = result
+                    print(f'Run: {run + 1:02d}, '
+                          f'Epoch: {epoch:02d}, '
+                          f'Loss: {loss:.4f}, '
+                          f'Train: {100 * train_acc:.2f}%, '
+                          f'Valid: {100 * valid_acc:.2f}% '
+                          f'Test: {100 * test_acc:.2f}%')
 
-        logger.print_statistics(run)
-    logger.print_statistics()
-
+            end_t = datetime.datetime.now()
+            logger.print_statistics(run)
+            print("model run ", run, " train time CPU=", end_t - start_t, " sec.")
+            print(getrusage(RUSAGE_SELF))
+        logger.print_statistics()
+        torch.save(model.state_dict(), "ogbn-mag-FM-GCN.model")
 
 if __name__ == "__main__":
     main()
Index: TSV_TO_Hetro_OGB.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TSV_TO_Hetro_OGB.py b/TSV_TO_Hetro_OGB.py
new file mode 100755
--- /dev/null	(date 1649394413385)
+++ b/TSV_TO_Hetro_OGB.py	(date 1649394413385)
@@ -0,0 +1,237 @@
+import  pandas as pd
+import gzip
+import os
+from sklearn.metrics import precision_recall_fscore_support as score
+def compress_gz(f_path):
+    f_in = open(f_path,'rb')
+    f_out = gzip.open(f_path+".gz", 'wb')
+    f_out.writelines(f_in)
+    f_out.close()
+    f_in.close()
+###################### Zip Folder to OGB Format
+#zip -r mag_ComputerProgramming_papers_venue_QM3.zip mag_ComputerProgramming_papers_venue_QM3/ -i '*.gz'
+
+if __name__ == '__main__':
+    csv_path="/media/hussein/UbuntuData/GithubRepos/KG-EaaS/OGBN-Data/ogb-mag.tsv"
+    # csv_path = "/media/hussein/UbuntuData/GithubRepos/KG-EaaS/OGBN-Data/mag_AtmosphericSciences_papers_venue_QM1.csv"
+    # csv_path = "/media/hussein/UbuntuData/GithubRepos/KG-EaaS/OGBN-Data/mag_Diffraction_papers_venue_QM2.csv"
+    # csv_path = "/media/hussein/UbuntuData/GithubRepos/KG-EaaS/OGBN-Data/mag_ComputerProgramming_papers_venue_QM3.csv"
+    # csv_path = "/media/hussein/UbuntuData/GithubRepos/KG-EaaS/OGBN-Data/mag_QuantumElectrodynamics_papers_venue_QM4.csv"
+    split_rel="http://mag.graph/has_year"
+    target_rel = "http://mag.graph/has_venue"
+    label_node="paper"
+    dataset_name=csv_path.split("/")[-1].split(".")[0]
+    split_by={"folder_name":"time","split_data_type":"int","train":2017,"valid":2018,"test":2019}
+    if csv_path.endswith(".tsv"):
+        g_tsv_df=pd.read_csv(csv_path,sep="\t")
+    else:
+        g_tsv_df = pd.read_csv(csv_path)
+    try:
+        g_tsv_df=g_tsv_df.rename(columns={"subject":"s","predicate":"p","object":"o"})
+    except:
+        print("g_tsv_df columns=",g_tsv_df.columns())
+    ########################delete non target papers #####################
+    lst_targets=g_tsv_df[g_tsv_df["p"]==target_rel]["s"].tolist()
+    cites_df=g_tsv_df[g_tsv_df["p"]=="http://mag.graph/cites"]
+    to_delete_papers=cites_df[~cites_df["o"].isin(lst_targets)]["o"].tolist()
+    g_tsv_df=g_tsv_df[~g_tsv_df["o"].isin(to_delete_papers)]
+    #####################################################################
+    relations_lst=g_tsv_df["p"].unique().tolist()
+    relations_lst.remove(split_rel)
+    relations_lst.remove(target_rel)
+    ################################write relations index ########################
+    relations_df=pd.DataFrame(relations_lst, columns=["rel name"])
+    relations_df["rel name"]=relations_df["rel name"].apply(lambda x: str(x).split("/")[-1])
+    relations_df["rel idx"]=relations_df.index
+    relations_df=relations_df[["rel idx","rel name"]]
+    map_folder = dataset_name+"/mapping"
+    try:
+        os.stat(map_folder)
+    except:
+        os.makedirs(map_folder)
+    relations_df.to_csv(map_folder+"/relidx2relname.csv",index=None)
+    compress_gz(map_folder+"/relidx2relname.csv")
+    ############################### create label index ########################
+    label_idx_df= pd.DataFrame(g_tsv_df[g_tsv_df["p"] == target_rel]["o"].apply(lambda x: str(x).strip()).unique().tolist(),columns=["label name"])
+    try:
+        label_idx_df["label name"] = label_idx_df["label name"].astype("int64")
+        label_idx_df = label_idx_df.sort_values(by=["label name"]).reset_index(drop=True)
+    except:
+        label_idx_df["label name"]=label_idx_df["label name"].astype("str")
+        label_idx_df = label_idx_df.sort_values(by=["label name"]).reset_index(drop=True)
+
+    label_idx_df["label idx"]=label_idx_df.index
+    label_idx_df=label_idx_df[["label idx","label name"]]
+    label_idx_df.to_csv(map_folder+"/labelidx2labelname.csv",index=None)
+    compress_gz(map_folder+"/labelidx2labelname.csv")
+    ###########################################prepare relations mapping#################################
+    relations_entites_map={}
+    relations_dic={}
+    entites_dic={}
+    for rel in relations_lst:
+        relations_dic[rel]=g_tsv_df[g_tsv_df["p"]==rel].reset_index(drop=True)
+        e1=str(relations_dic[rel]["s"][0]).split("/")
+        e1=e1[len(e1)-2]
+        e2 = str(relations_dic[rel]["o"][0]).split("/")
+        e2 = e2[len(e2) - 2]
+        relations_entites_map[rel]=(e1,rel,e2)
+        if e1 in entites_dic:
+            entites_dic[e1]=entites_dic[e1].union(set(relations_dic[rel]["s"].apply(lambda x:str(x).split("/")[-1]).unique()))
+        else:
+            entites_dic[e1] = set(relations_dic[rel]["s"].apply(lambda x:str(x).split("/")[-1]).unique())
+
+        if e2 in entites_dic:
+            entites_dic[e2] = entites_dic[e2].union(set(relations_dic[rel]["o"].apply(lambda x:str(x).split("/")[-1]).unique()))
+        else:
+            entites_dic[e2] = set(relations_dic[rel]["o"].apply(lambda x:str(x).split("/")[-1]).unique())
+
+    ############################ write entites index #################################
+    for key in list(entites_dic.keys()) :
+        entites_dic[key]=pd.DataFrame(list(entites_dic[key]), columns=['ent name']).astype('int64').sort_values(by="ent name").reset_index(drop=True)
+        entites_dic[key]=entites_dic[key].drop_duplicates()
+        entites_dic[key]["ent idx"]=entites_dic[key].index
+        entites_dic[key] = entites_dic[key][["ent idx","ent name"]]
+        entites_dic[key+"_dic"]=pd.Series(entites_dic[key]["ent idx"].values,index=entites_dic[key]["ent name"]).to_dict()
+        # print("key=",entites_dic[key+"_dic"])
+        map_folder=dataset_name+"/mapping"
+        try:
+            os.stat(map_folder)
+        except:
+            os.makedirs(map_folder)
+        entites_dic[key].to_csv(map_folder+"/"+key+"_entidx2name.csv",index=None)
+        compress_gz(map_folder+"/"+key+"_entidx2name.csv")
+    #################### write nodes statistics ######################
+    lst_node_has_feat= [list(filter(lambda entity: str(entity).endswith("_dic") == False, list(entites_dic.keys())))]
+    lst_node_has_label=lst_node_has_feat.copy()
+    lst_num_node_dict = lst_node_has_feat.copy()
+    lst_has_feat = []
+    lst_has_label=[]
+    lst_num_node=[]
+
+    for entity in lst_node_has_feat[0]:
+        if str(entity)== str(label_node):
+            lst_has_label.append("True")
+            lst_has_feat.append("True")
+        else:
+            lst_has_label.append("False")
+            lst_has_feat.append("False")
+
+        # lst_has_feat.append("False")
+        lst_num_node.append( len(entites_dic[entity+"_dic"]))
+
+    lst_node_has_feat.append(lst_has_feat)
+    lst_node_has_label.append(lst_has_label)
+    lst_num_node_dict.append(lst_num_node)
+
+    lst_relations=[]
+
+    for k in list(relations_entites_map.keys()):
+        (e1,rel,e2)=relations_entites_map[k]
+        lst_relations.append([e1,str(rel).split("/")[-1],e2])
+
+    map_folder = dataset_name + "/raw"
+    try:
+        os.stat(map_folder)
+    except:
+        os.makedirs(map_folder)
+
+    pd.DataFrame(lst_node_has_feat).to_csv(dataset_name + "/raw/nodetype-has-feat.csv", header=None, index=None)
+    compress_gz(dataset_name + "/raw/nodetype-has-feat.csv")
+
+    pd.DataFrame(lst_node_has_label).to_csv(dataset_name + "/raw/nodetype-has-label.csv", header=None, index=None)
+    compress_gz(dataset_name + "/raw/nodetype-has-label.csv")
+
+    pd.DataFrame(lst_num_node_dict).to_csv(dataset_name + "/raw/num-node-dict.csv", header=None, index=None)
+    compress_gz(dataset_name + "/raw/num-node-dict.csv")
+
+    pd.DataFrame(lst_relations).to_csv(dataset_name + "/raw/triplet-type-list.csv", header=None, index=None)
+    compress_gz(dataset_name + "/raw/triplet-type-list.csv")
+    ############################### create label relation index ######################
+    label_idx_dic = pd.Series(label_idx_df["label idx"].values, index=label_idx_df["label name"]).to_dict()
+    labels_rel_df = g_tsv_df[g_tsv_df["p"] == target_rel]
+    label_type = str(labels_rel_df["s"].values[0]).split("/")
+    label_type=label_type[len(label_type)-2]
+
+    labels_rel_df["s_idx"]=labels_rel_df["s"].apply(lambda x: str(x).split("/")[-1])
+    labels_rel_df["s_idx"] = labels_rel_df["s_idx"].astype("int64")
+    labels_rel_df["s_idx"]=labels_rel_df["s_idx"].apply(lambda x: entites_dic[label_type+"_dic"][int(x)])
+    labels_rel_df=labels_rel_df.sort_values(by=["s_idx"]).reset_index(drop=True)
+    labels_rel_df["o_idx"]=labels_rel_df["o"].apply(lambda x: str(x).split("/")[-1])
+    labels_rel_df["o_idx"]=labels_rel_df["o_idx"].apply(lambda x:label_idx_dic[int(x)])
+    out_labels_df=labels_rel_df[["o_idx"]]
+    map_folder = dataset_name + "/raw/node-label/"+label_type
+    try:
+        os.stat(map_folder)
+    except:
+        os.makedirs(map_folder)
+    out_labels_df.to_csv(map_folder+"/node-label.csv",header=None,index=None)
+    compress_gz(map_folder+"/node-label.csv")
+    ###########################################split parts (train/test/validate)#########################
+    split_df = g_tsv_df[g_tsv_df["p"] == split_rel]
+    label_type = str(split_df["s"].values[0]).split("/")
+    label_type = label_type[len(label_type) - 2]
+    try:
+        split_df["s"] = split_df["s"].apply(lambda x: str(x).split("/")[-1]).astype("int64").apply(lambda x:entites_dic[label_type+"_dic"][x])
+    except:
+        split_df["s"] = split_df["s"].apply(lambda x: str(x).split("/")[-1]).astype("str").apply(lambda x: entites_dic[label_type + "_dic"][int(x)])
+    split_df["o"]=split_df["o"].astype(split_by["split_data_type"])
+    split_df=split_df.sort_values(by=["s"]).reset_index(drop=True)
+    train_df = split_df[split_df["o"] <= split_by["train"]]["s"]
+    valid_df = split_df[(split_df["o"] > split_by["train"]) & (split_df["o"] <= split_by["valid"])]["s"]
+    test_df = split_df[ (split_df["o"] > split_by["valid"]) & (split_df["o"] <= split_by["test"])]["s"]
+
+    map_folder = dataset_name + "/split/"+ split_by["folder_name"]+"/"+label_type
+    try:
+        os.stat(map_folder)
+    except:
+        os.makedirs(map_folder)
+    train_df.to_csv(map_folder + "/train.csv", index=None,header=None)
+    compress_gz(map_folder + "/train.csv")
+    valid_df.to_csv(map_folder + "/valid.csv", index=None, header=None)
+    compress_gz(map_folder + "/valid.csv")
+    test_df.to_csv(map_folder + "/test.csv", index=None, header=None)
+    compress_gz(map_folder + "/test.csv")
+    ###################### create nodetype-has-split.csv#####################
+    lst_node_has_split=[ list(filter(lambda entity: str(entity).endswith("_dic")==False, list(entites_dic.keys())))]
+    lst_has_split=[]
+    for rel in lst_node_has_split[0]:
+        if rel==label_type:
+            lst_has_split.append("True")
+        else:
+            lst_has_split.append("False")
+    lst_node_has_split.append(lst_has_split)
+    pd.DataFrame(lst_node_has_split).to_csv(dataset_name + "/split/"+ split_by["folder_name"]+"/nodetype-has-split.csv",header=None,index=None)
+    compress_gz(dataset_name + "/split/"+ split_by["folder_name"]+"/nodetype-has-split.csv")
+    ############################ write entites relations  #################################
+    idx=0
+    for key in entites_dic["author_dic"].keys():
+        print(key, entites_dic["author_dic"][key])
+        idx=idx+1
+        if idx>10:
+            break;
+    print( list(entites_dic.keys()))
+    for rel in relations_dic:
+        e1,rel,e2=relations_entites_map[rel]
+        relations_dic[rel]["s_idx"]=relations_dic[rel]["s"].apply(lambda x:entites_dic[e1+"_dic"][int(str(x).split("/")[-1])] ).astype("int64")
+        relations_dic[rel]["o_idx"] = relations_dic[rel]["o"].apply(lambda x: entites_dic[e2 + "_dic"][int(str(x).split("/")[-1])]).astype("int64")
+        relations_dic[rel]=relations_dic[rel].sort_values(by="s_idx").reset_index(drop=True)
+        rel_out=relations_dic[rel].drop(columns=["s","p","o"])
+        map_folder = dataset_name+"/raw/relations/"+e1+"___"+rel.split("/")[-1]+"___"+e2
+        try:
+            os.stat(map_folder)
+        except:
+            os.makedirs(map_folder)
+        rel_out.to_csv(map_folder + "/edge.csv", index=None, header=None)
+        compress_gz(map_folder + "/edge.csv")
+        ########## write relations num #################
+        f = open(map_folder+"/num-edge-list.csv", "w")
+        f.write(str(len(relations_dic[rel])))
+        f.close()
+        compress_gz(map_folder+"/num-edge-list.csv")
+        ##################### write relations idx #######################
+        rel_idx=relations_df[relations_df["rel name"]==rel.split("/")[-1]]["rel idx"].values[0]
+        rel_out["rel_idx"]=rel_idx
+        rel_idx_df=rel_out["rel_idx"]
+        rel_idx_df.to_csv(map_folder+"/edge_reltype.csv",header=None,index=None)
+        compress_gz(map_folder+"/edge_reltype.csv")
+    # print(entites_dic)
Index: examples/nodeproppred/products/gnn.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import argparse\n\nimport torch\nimport torch.nn.functional as F\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv, SAGEConv\n\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n\nfrom logger import Logger\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout):\n        super(GCN, self).__init__()\n\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(\n            GCNConv(in_channels, hidden_channels, normalize=False))\n        for _ in range(num_layers - 2):\n            self.convs.append(\n                GCNConv(hidden_channels, hidden_channels, normalize=False))\n        self.convs.append(\n            GCNConv(hidden_channels, out_channels, normalize=False))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x, adj_t):\n        for conv in self.convs[:-1]:\n            x = conv(x, adj_t)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, adj_t)\n        return torch.log_softmax(x, dim=-1)\n\n\nclass SAGE(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout):\n        super(SAGE, self).__init__()\n\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(SAGEConv(in_channels, hidden_channels))\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n        self.convs.append(SAGEConv(hidden_channels, out_channels))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x, adj_t):\n        for conv in self.convs[:-1]:\n            x = conv(x, adj_t)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, adj_t)\n        return torch.log_softmax(x, dim=-1)\n\n\ndef train(model, data, train_idx, optimizer):\n    model.train()\n\n    optimizer.zero_grad()\n    out = model(data.x, data.adj_t)[train_idx]\n    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n\n\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    model.eval()\n\n    out = model(data.x, data.adj_t)\n    y_pred = out.argmax(dim=-1, keepdim=True)\n\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n\n    return train_acc, valid_acc, test_acc\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='OGBN-Products (GNN)')\n    parser.add_argument('--device', type=int, default=0)\n    parser.add_argument('--log_steps', type=int, default=1)\n    parser.add_argument('--use_sage', action='store_true')\n    parser.add_argument('--num_layers', type=int, default=3)\n    parser.add_argument('--hidden_channels', type=int, default=256)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--lr', type=float, default=0.01)\n    parser.add_argument('--epochs', type=int, default=300)\n    parser.add_argument('--runs', type=int, default=10)\n    args = parser.parse_args()\n    print(args)\n\n    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n\n    dataset = PygNodePropPredDataset(name='ogbn-products',\n                                     transform=T.ToSparseTensor())\n    data = dataset[0]\n\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx['train'].to(device)\n\n    if args.use_sage:\n        model = SAGE(data.num_features, args.hidden_channels,\n                     dataset.num_classes, args.num_layers,\n                     args.dropout).to(device)\n    else:\n        model = GCN(data.num_features, args.hidden_channels,\n                    dataset.num_classes, args.num_layers,\n                    args.dropout).to(device)\n\n        # Pre-compute GCN normalization.\n        adj_t = data.adj_t.set_diag()\n        deg = adj_t.sum(dim=1).to(torch.float)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n        data.adj_t = adj_t\n\n    data = data.to(device)\n\n    evaluator = Evaluator(name='ogbn-products')\n    logger = Logger(args.runs, args)\n\n    for run in range(args.runs):\n        model.reset_parameters()\n        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n        for epoch in range(1, 1 + args.epochs):\n            loss = train(model, data, train_idx, optimizer)\n            result = test(model, data, split_idx, evaluator)\n            logger.add_result(run, result)\n\n            if epoch % args.log_steps == 0:\n                train_acc, valid_acc, test_acc = result\n                print(f'Run: {run + 1:02d}, '\n                      f'Epoch: {epoch:02d}, '\n                      f'Loss: {loss:.4f}, '\n                      f'Train: {100 * train_acc:.2f}%, '\n                      f'Valid: {100 * valid_acc:.2f}% '\n                      f'Test: {100 * test_acc:.2f}%')\n\n        logger.print_statistics(run)\n    logger.print_statistics()\n\n\nif __name__ == \"__main__\":\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/examples/nodeproppred/products/gnn.py b/examples/nodeproppred/products/gnn.py
--- a/examples/nodeproppred/products/gnn.py	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/examples/nodeproppred/products/gnn.py	(date 1649353569214)
@@ -105,7 +105,8 @@
     parser = argparse.ArgumentParser(description='OGBN-Products (GNN)')
     parser.add_argument('--device', type=int, default=0)
     parser.add_argument('--log_steps', type=int, default=1)
-    parser.add_argument('--use_sage', action='store_true')
+    # parser.add_argument('--use_sage', action='store_true')
+    parser.add_argument('--use_sage', type=int,default=False)
     parser.add_argument('--num_layers', type=int, default=3)
     parser.add_argument('--hidden_channels', type=int, default=256)
     parser.add_argument('--dropout', type=float, default=0.5)
@@ -126,10 +127,12 @@
     train_idx = split_idx['train'].to(device)
 
     if args.use_sage:
+        print("SAGE")
         model = SAGE(data.num_features, args.hidden_channels,
                      dataset.num_classes, args.num_layers,
                      args.dropout).to(device)
     else:
+        print("GCN")
         model = GCN(data.num_features, args.hidden_channels,
                     dataset.num_classes, args.num_layers,
                     args.dropout).to(device)
Index: examples/nodeproppred/mag/graph_saint.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from copy import copy\nimport argparse\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import ModuleList, Linear, ParameterDict, Parameter\nfrom torch_sparse import SparseTensor\nfrom torch_geometric.utils import to_undirected\nfrom torch_geometric.data import Data, GraphSAINTRandomWalkSampler\nfrom torch_geometric.utils.hetero import group_hetero_graph\nfrom torch_geometric.nn import MessagePassing\n\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n\nfrom logger import Logger\n\nparser = argparse.ArgumentParser(description='OGBN-MAG (GraphSAINT)')\nparser.add_argument('--device', type=int, default=0)\nparser.add_argument('--num_layers', type=int, default=2)\nparser.add_argument('--hidden_channels', type=int, default=64)\nparser.add_argument('--dropout', type=float, default=0.5)\nparser.add_argument('--lr', type=float, default=0.005)\nparser.add_argument('--epochs', type=int, default=30)\nparser.add_argument('--runs', type=int, default=10)\nparser.add_argument('--batch_size', type=int, default=20000)\nparser.add_argument('--walk_length', type=int, default=2)\nparser.add_argument('--num_steps', type=int, default=30)\nargs = parser.parse_args()\nprint(args)\n\ndataset = PygNodePropPredDataset(name='ogbn-mag')\ndata = dataset[0]\nsplit_idx = dataset.get_idx_split()\nevaluator = Evaluator(name='ogbn-mag')\nlogger = Logger(args.runs, args)\n\n# We do not consider those attributes for now.\ndata.node_year_dict = None\ndata.edge_reltype_dict = None\n\nprint(data)\n\nedge_index_dict = data.edge_index_dict\n\n# We need to add reverse edges to the heterogeneous graph.\nr, c = edge_index_dict[('author', 'affiliated_with', 'institution')]\nedge_index_dict[('institution', 'to', 'author')] = torch.stack([c, r])\n\nr, c = edge_index_dict[('author', 'writes', 'paper')]\nedge_index_dict[('paper', 'to', 'author')] = torch.stack([c, r])\n\nr, c = edge_index_dict[('paper', 'has_topic', 'field_of_study')]\nedge_index_dict[('field_of_study', 'to', 'paper')] = torch.stack([c, r])\n\n# Convert to undirected paper <-> paper relation.\nedge_index = to_undirected(edge_index_dict[('paper', 'cites', 'paper')])\nedge_index_dict[('paper', 'cites', 'paper')] = edge_index\n\n# We convert the individual graphs into a single big one, so that sampling\n# neighbors does not need to care about different edge types.\n# This will return the following:\n# * `edge_index`: The new global edge connectivity.\n# * `edge_type`: The edge type for each edge.\n# * `node_type`: The node type for each node.\n# * `local_node_idx`: The original index for each node.\n# * `local2global`: A dictionary mapping original (local) node indices of\n#    type `key` to global ones.\n# `key2int`: A dictionary that maps original keys to their new canonical type.\nout = group_hetero_graph(data.edge_index_dict, data.num_nodes_dict)\nedge_index, edge_type, node_type, local_node_idx, local2global, key2int = out\n\nhomo_data = Data(edge_index=edge_index, edge_attr=edge_type,\n                 node_type=node_type, local_node_idx=local_node_idx,\n                 num_nodes=node_type.size(0))\n\nhomo_data.y = node_type.new_full((node_type.size(0), 1), -1)\nhomo_data.y[local2global['paper']] = data.y_dict['paper']\n\nhomo_data.train_mask = torch.zeros((node_type.size(0)), dtype=torch.bool)\nhomo_data.train_mask[local2global['paper'][split_idx['train']['paper']]] = True\n\nprint(homo_data)\n\ntrain_loader = GraphSAINTRandomWalkSampler(homo_data,\n                                           batch_size=args.batch_size,\n                                           walk_length=args.num_layers,\n                                           num_steps=args.num_steps,\n                                           sample_coverage=0,\n                                           save_dir=dataset.processed_dir)\n\n# Map informations to their canonical type.\nx_dict = {}\nfor key, x in data.x_dict.items():\n    x_dict[key2int[key]] = x\n\nnum_nodes_dict = {}\nfor key, N in data.num_nodes_dict.items():\n    num_nodes_dict[key2int[key]] = N\n\n\nclass RGCNConv(MessagePassing):\n    def __init__(self, in_channels, out_channels, num_node_types,\n                 num_edge_types):\n        super(RGCNConv, self).__init__(aggr='mean')\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_node_types = num_node_types\n        self.num_edge_types = num_edge_types\n\n        self.rel_lins = ModuleList([\n            Linear(in_channels, out_channels, bias=False)\n            for _ in range(num_edge_types)\n        ])\n\n        self.root_lins = ModuleList([\n            Linear(in_channels, out_channels, bias=True)\n            for _ in range(num_node_types)\n        ])\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for lin in self.rel_lins:\n            lin.reset_parameters()\n        for lin in self.root_lins:\n            lin.reset_parameters()\n\n    def forward(self, x, edge_index, edge_type, node_type):\n        out = x.new_zeros(x.size(0), self.out_channels)\n\n        for i in range(self.num_edge_types):\n            mask = edge_type == i\n            out.add_(self.propagate(edge_index[:, mask], x=x, edge_type=i))\n\n        for i in range(self.num_node_types):\n            mask = node_type == i\n            out[mask] += self.root_lins[i](x[mask])\n\n        return out\n\n    def message(self, x_j, edge_type: int):\n        return self.rel_lins[edge_type](x_j)\n\n\nclass RGCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout, num_nodes_dict, x_types, num_edge_types):\n        super(RGCN, self).__init__()\n\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        node_types = list(num_nodes_dict.keys())\n        num_node_types = len(node_types)\n\n        self.num_node_types = num_node_types\n        self.num_edge_types = num_edge_types\n\n        # Create embeddings for all node types that do not come with features.\n        self.emb_dict = ParameterDict({\n            f'{key}': Parameter(torch.Tensor(num_nodes_dict[key], in_channels))\n            for key in set(node_types).difference(set(x_types))\n        })\n\n        I, H, O = in_channels, hidden_channels, out_channels  # noqa\n\n        # Create `num_layers` many message passing layers.\n        self.convs = ModuleList()\n        self.convs.append(RGCNConv(I, H, num_node_types, num_edge_types))\n        for _ in range(num_layers - 2):\n            self.convs.append(RGCNConv(H, H, num_node_types, num_edge_types))\n        self.convs.append(RGCNConv(H, O, self.num_node_types, num_edge_types))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for emb in self.emb_dict.values():\n            torch.nn.init.xavier_uniform_(emb)\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def group_input(self, x_dict, node_type, local_node_idx):\n        # Create global node feature matrix.\n        h = torch.zeros((node_type.size(0), self.in_channels),\n                        device=node_type.device)\n\n        for key, x in x_dict.items():\n            mask = node_type == key\n            h[mask] = x[local_node_idx[mask]]\n\n        for key, emb in self.emb_dict.items():\n            mask = node_type == int(key)\n            h[mask] = emb[local_node_idx[mask]]\n\n        return h\n\n    def forward(self, x_dict, edge_index, edge_type, node_type,\n                local_node_idx):\n\n        x = self.group_input(x_dict, node_type, local_node_idx)\n\n        for i, conv in enumerate(self.convs):\n            x = conv(x, edge_index, edge_type, node_type)\n            if i != self.num_layers - 1:\n                x = F.relu(x)\n                x = F.dropout(x, p=0.5, training=self.training)\n\n        return x.log_softmax(dim=-1)\n\n    def inference(self, x_dict, edge_index_dict, key2int):\n        # We can perform full-batch inference on GPU.\n\n        device = list(x_dict.values())[0].device\n\n        x_dict = copy(x_dict)\n        for key, emb in self.emb_dict.items():\n            x_dict[int(key)] = emb\n\n        adj_t_dict = {}\n        for key, (row, col) in edge_index_dict.items():\n            adj_t_dict[key] = SparseTensor(row=col, col=row).to(device)\n\n        for i, conv in enumerate(self.convs):\n            out_dict = {}\n\n            for j, x in x_dict.items():\n                out_dict[j] = conv.root_lins[j](x)\n\n            for keys, adj_t in adj_t_dict.items():\n                src_key, target_key = keys[0], keys[-1]\n                out = out_dict[key2int[target_key]]\n                tmp = adj_t.matmul(x_dict[key2int[src_key]], reduce='mean')\n                out.add_(conv.rel_lins[key2int[keys]](tmp))\n\n            if i != self.num_layers - 1:\n                for j in range(self.num_node_types):\n                    F.relu_(out_dict[j])\n\n            x_dict = out_dict\n\n        return x_dict\n\n\ndevice = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n\nmodel = RGCN(128, args.hidden_channels, dataset.num_classes, args.num_layers,\n             args.dropout, num_nodes_dict, list(x_dict.keys()),\n             len(edge_index_dict.keys())).to(device)\n\nx_dict = {k: v.to(device) for k, v in x_dict.items()}\n\n\ndef train(epoch):\n    model.train()\n\n    pbar = tqdm(total=args.num_steps * args.batch_size)\n    pbar.set_description(f'Epoch {epoch:02d}')\n\n    total_loss = total_examples = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(x_dict, data.edge_index, data.edge_attr, data.node_type,\n                    data.local_node_idx)\n        out = out[data.train_mask]\n        y = data.y[data.train_mask].squeeze()\n        loss = F.nll_loss(out, y)\n        loss.backward()\n        optimizer.step()\n\n        num_examples = data.train_mask.sum().item()\n        total_loss += loss.item() * num_examples\n        total_examples += num_examples\n        pbar.update(args.batch_size)\n\n    pbar.close()\n\n    return total_loss / total_examples\n\n\n@torch.no_grad()\ndef test():\n    model.eval()\n\n    out = model.inference(x_dict, edge_index_dict, key2int)\n    out = out[key2int['paper']]\n\n    y_pred = out.argmax(dim=-1, keepdim=True).cpu()\n    y_true = data.y_dict['paper']\n\n    train_acc = evaluator.eval({\n        'y_true': y_true[split_idx['train']['paper']],\n        'y_pred': y_pred[split_idx['train']['paper']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': y_true[split_idx['valid']['paper']],\n        'y_pred': y_pred[split_idx['valid']['paper']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': y_true[split_idx['test']['paper']],\n        'y_pred': y_pred[split_idx['test']['paper']],\n    })['acc']\n\n    return train_acc, valid_acc, test_acc\n\n\ntest()  # Test if inference on GPU succeeds.\nfor run in range(args.runs):\n    model.reset_parameters()\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n    for epoch in range(1, 1 + args.epochs):\n        loss = train(epoch)\n        torch.cuda.empty_cache()\n        result = test()\n        logger.add_result(run, result)\n        train_acc, valid_acc, test_acc = result\n        print(f'Run: {run + 1:02d}, '\n              f'Epoch: {epoch:02d}, '\n              f'Loss: {loss:.4f}, '\n              f'Train: {100 * train_acc:.2f}%, '\n              f'Valid: {100 * valid_acc:.2f}%, '\n              f'Test: {100 * test_acc:.2f}%')\n    logger.print_statistics(run)\nlogger.print_statistics()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/examples/nodeproppred/mag/graph_saint.py b/examples/nodeproppred/mag/graph_saint.py
--- a/examples/nodeproppred/mag/graph_saint.py	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/examples/nodeproppred/mag/graph_saint.py	(date 1649361373798)
@@ -1,7 +1,9 @@
 from copy import copy
 import argparse
+
+import pandas
 from tqdm import tqdm
-
+import datetime
 import torch
 import torch.nn.functional as F
 from torch.nn import ModuleList, Linear, ParameterDict, Parameter
@@ -12,7 +14,7 @@
 from torch_geometric.nn import MessagePassing
 
 from ogb.nodeproppred import PygNodePropPredDataset, Evaluator
-
+from resource import *
 from logger import Logger
 
 parser = argparse.ArgumentParser(description='OGBN-MAG (GraphSAINT)')
@@ -22,19 +24,28 @@
 parser.add_argument('--dropout', type=float, default=0.5)
 parser.add_argument('--lr', type=float, default=0.005)
 parser.add_argument('--epochs', type=int, default=30)
-parser.add_argument('--runs', type=int, default=10)
+parser.add_argument('--runs', type=int, default=1)
 parser.add_argument('--batch_size', type=int, default=20000)
 parser.add_argument('--walk_length', type=int, default=2)
 parser.add_argument('--num_steps', type=int, default=30)
+parser.add_argument('--loadTrainedModel', type=int, default=1)
 args = parser.parse_args()
 print(args)
-
+print(getrusage(RUSAGE_SELF))
+start_t = datetime.datetime.now()
+# dataset = PygNodePropPredDataset(name='ogbn-mag-QM1')
+# dataset = PygNodePropPredDataset(name='ogbn-mag-QM2')
+# dataset = PygNodePropPredDataset(name='ogbn-mag-QM3')
 dataset = PygNodePropPredDataset(name='ogbn-mag')
+# dataset = PygNodePropPredDataset(name='ogbn-mag-QM4')
 data = dataset[0]
 split_idx = dataset.get_idx_split()
+end_t = datetime.datetime.now()
+print("dataset init time=", end_t - start_t, " sec.")
 evaluator = Evaluator(name='ogbn-mag')
 logger = Logger(args.runs, args)
 
+start_t = datetime.datetime.now()
 # We do not consider those attributes for now.
 data.node_year_dict = None
 data.edge_reltype_dict = None
@@ -44,18 +55,22 @@
 edge_index_dict = data.edge_index_dict
 
 # We need to add reverse edges to the heterogeneous graph.
-r, c = edge_index_dict[('author', 'affiliated_with', 'institution')]
-edge_index_dict[('institution', 'to', 'author')] = torch.stack([c, r])
+if ('author', 'affiliated_with', 'institution') in edge_index_dict.keys():
+    r, c = edge_index_dict[('author', 'affiliated_with', 'institution')]
+    edge_index_dict[('institution', 'to', 'author')] = torch.stack([c, r])
 
-r, c = edge_index_dict[('author', 'writes', 'paper')]
-edge_index_dict[('paper', 'to', 'author')] = torch.stack([c, r])
+if ('author', 'writes', 'paper') in edge_index_dict.keys():
+    r, c = edge_index_dict[('author', 'writes', 'paper')]
+    edge_index_dict[('paper', 'to', 'author')] = torch.stack([c, r])
 
-r, c = edge_index_dict[('paper', 'has_topic', 'field_of_study')]
-edge_index_dict[('field_of_study', 'to', 'paper')] = torch.stack([c, r])
+if ('paper', 'has_topic', 'field_of_study') in edge_index_dict.keys():
+    r, c = edge_index_dict[('paper', 'has_topic', 'field_of_study')]
+    edge_index_dict[('field_of_study', 'to', 'paper')] = torch.stack([c, r])
 
 # Convert to undirected paper <-> paper relation.
-edge_index = to_undirected(edge_index_dict[('paper', 'cites', 'paper')])
-edge_index_dict[('paper', 'cites', 'paper')] = edge_index
+if ('paper', 'cites', 'paper') in edge_index_dict.keys():
+    edge_index = to_undirected(edge_index_dict[('paper', 'cites', 'paper')])
+    edge_index_dict[('paper', 'cites', 'paper')] = edge_index
 
 # We convert the individual graphs into a single big one, so that sampling
 # neighbors does not need to care about different edge types.
@@ -90,15 +105,22 @@
                                            save_dir=dataset.processed_dir)
 
 # Map informations to their canonical type.
+#######################intialize random features ###############################
+feat = torch.Tensor(data.num_nodes_dict['paper'], 128)
+torch.nn.init.xavier_uniform_(feat)
+feat_dic = {'paper':feat}
+################################################################
 x_dict = {}
-for key, x in data.x_dict.items():
+# for key, x in data.x_dict.items():
+for key, x in feat_dic.items():
     x_dict[key2int[key]] = x
 
 num_nodes_dict = {}
 for key, N in data.num_nodes_dict.items():
     num_nodes_dict[key2int[key]] = N
 
-
+end_t = datetime.datetime.now()
+print("model init time CPU=", end_t - start_t, " sec.")
 class RGCNConv(MessagePassing):
     def __init__(self, in_channels, out_channels, num_node_types,
                  num_edge_types):
@@ -253,7 +275,7 @@
              len(edge_index_dict.keys())).to(device)
 
 x_dict = {k: v.to(device) for k, v in x_dict.items()}
-
+optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
 
 def train(epoch):
     model.train()
@@ -308,22 +330,41 @@
 
     return train_acc, valid_acc, test_acc
 
+if args.loadTrainedModel==1:
+    model.load_state_dict(torch.load("ogbn-mag-FM-GSaint.model"))
+    model.eval()
+    out = model.inference(x_dict, edge_index_dict, key2int)
+    out = out[key2int['paper']]
+    y_pred = out.argmax(dim=-1, keepdim=True).cpu()
+    y_true = data.y_dict['paper']
 
-test()  # Test if inference on GPU succeeds.
-for run in range(args.runs):
-    model.reset_parameters()
-    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
-    for epoch in range(1, 1 + args.epochs):
-        loss = train(epoch)
-        torch.cuda.empty_cache()
-        result = test()
-        logger.add_result(run, result)
-        train_acc, valid_acc, test_acc = result
-        print(f'Run: {run + 1:02d}, '
-              f'Epoch: {epoch:02d}, '
-              f'Loss: {loss:.4f}, '
-              f'Train: {100 * train_acc:.2f}%, '
-              f'Valid: {100 * valid_acc:.2f}%, '
-              f'Test: {100 * test_acc:.2f}%')
-    logger.print_statistics(run)
-logger.print_statistics()
+    out_lst=torch.flatten(y_true).tolist()
+    pred_lst = torch.flatten(y_pred).tolist()
+    out_df = pandas.DataFrame({"y_pred":pred_lst,"y_true":out_lst})
+    # print(y_pred, data.y_dict['paper'])
+    # print(out_df)
+    out_df.to_csv("GSaint_mag_output.csv",index=None)
+else:
+    test()  # Test if inference on GPU succeeds.
+    for run in range(args.runs):
+        start_t = datetime.datetime.now()
+        model.reset_parameters()
+
+        for epoch in range(1, 1 + args.epochs):
+            loss = train(epoch)
+            torch.cuda.empty_cache()
+            result = test()
+            logger.add_result(run, result)
+            train_acc, valid_acc, test_acc = result
+            print(f'Run: {run + 1:02d}, '
+                  f'Epoch: {epoch:02d}, '
+                  f'Loss: {loss:.4f}, '
+                  f'Train: {100 * train_acc:.2f}%, '
+                  f'Valid: {100 * valid_acc:.2f}%, '
+                  f'Test: {100 * test_acc:.2f}%')
+        logger.print_statistics(run)
+        end_t = datetime.datetime.now()
+        print("model run ", run, " train time CPU=", end_t - start_t, " sec.")
+        print(getrusage(RUSAGE_SELF))
+    logger.print_statistics()
+    torch.save(model.state_dict(), "ogbn-mag-FM-GSaint.model")
Index: examples/linkproppred/biokg/run.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/python3\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport logging\nimport os\nimport random\n\nimport numpy as np\nimport torch\n\nfrom torch.utils.data import DataLoader\n\nfrom model import KGEModel\n\nfrom dataloader import TrainDataset\nfrom dataloader import BidirectionalOneShotIterator\n\nfrom ogb.linkproppred import LinkPropPredDataset, Evaluator\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport time\nfrom tensorboardX import SummaryWriter\nimport pdb\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser(\n        description='Training and Testing Knowledge Graph Embedding Models',\n        usage='train.py [<args>] [-h | --help]'\n    )\n\n    parser.add_argument('--cuda', action='store_true', help='use GPU')\n    \n    parser.add_argument('--do_train', action='store_true')\n    parser.add_argument('--do_valid', action='store_true')\n    parser.add_argument('--do_test', action='store_true')\n    parser.add_argument('--evaluate_train', action='store_true', help='Evaluate on training data')\n    \n    parser.add_argument('--dataset', type=str, default='ogbl-biokg', help='dataset name, default to biokg')\n    parser.add_argument('--model', default='TransE', type=str)\n    parser.add_argument('-de', '--double_entity_embedding', action='store_true')\n    parser.add_argument('-dr', '--double_relation_embedding', action='store_true')\n    \n    parser.add_argument('-n', '--negative_sample_size', default=128, type=int)\n    parser.add_argument('-d', '--hidden_dim', default=500, type=int)\n    parser.add_argument('-g', '--gamma', default=12.0, type=float)\n    parser.add_argument('-adv', '--negative_adversarial_sampling', action='store_true')\n    parser.add_argument('-a', '--adversarial_temperature', default=1.0, type=float)\n    parser.add_argument('-b', '--batch_size', default=1024, type=int)\n    parser.add_argument('-r', '--regularization', default=0.0, type=float)\n    parser.add_argument('--test_batch_size', default=4, type=int, help='valid/test batch size')\n    parser.add_argument('--uni_weight', action='store_true', \n                        help='Otherwise use subsampling weighting like in word2vec')\n    \n    parser.add_argument('-lr', '--learning_rate', default=0.0001, type=float)\n    parser.add_argument('-cpu', '--cpu_num', default=10, type=int)\n    parser.add_argument('-init', '--init_checkpoint', default=None, type=str)\n    parser.add_argument('-save', '--save_path', default=None, type=str)\n    parser.add_argument('--max_steps', default=100000, type=int)\n    parser.add_argument('--warm_up_steps', default=None, type=int)\n    \n    parser.add_argument('--save_checkpoint_steps', default=10000, type=int)\n    parser.add_argument('--valid_steps', default=10000, type=int)\n    parser.add_argument('--log_steps', default=100, type=int, help='train log every xx steps')\n    parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')\n    \n    parser.add_argument('--nentity', type=int, default=0, help='DO NOT MANUALLY SET')\n    parser.add_argument('--nrelation', type=int, default=0, help='DO NOT MANUALLY SET')\n    \n    parser.add_argument('--print_on_screen', action='store_true', help='log on screen or not')\n    parser.add_argument('--ntriples_eval_train', type=int, default=200000, help='number of training triples to evaluate eventually')\n    parser.add_argument('--neg_size_eval_train', type=int, default=500, help='number of negative samples when evaluating training triples')\n    return parser.parse_args(args)\n\ndef override_config(args):\n    '''\n    Override model and data configuration\n    '''\n    \n    with open(os.path.join(args.init_checkpoint, 'config.json'), 'r') as fjson:\n        argparse_dict = json.load(fjson)\n    \n    args.dataset = argparse_dict['dataset']\n    args.model = argparse_dict['model']\n    args.double_entity_embedding = argparse_dict['double_entity_embedding']\n    args.double_relation_embedding = argparse_dict['double_relation_embedding']\n    args.hidden_dim = argparse_dict['hidden_dim']\n    args.test_batch_size = argparse_dict['test_batch_size']\n    \ndef save_model(model, optimizer, save_variable_list, args):\n    '''\n    Save the parameters of the model and the optimizer,\n    as well as some other variables such as step and learning_rate\n    '''\n    \n    argparse_dict = vars(args)\n    with open(os.path.join(args.save_path, 'config.json'), 'w') as fjson:\n        json.dump(argparse_dict, fjson)\n\n    save_dict = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()}\n    save_dict.update(save_variable_list)\n    torch.save(save_dict, os.path.join(args.save_path, 'checkpoint'))\n    \n    entity_embedding = model.entity_embedding.detach().cpu().numpy()\n    np.save(\n        os.path.join(args.save_path, 'entity_embedding'), \n        entity_embedding\n    )\n    \n    relation_embedding = model.relation_embedding.detach().cpu().numpy()\n    np.save(\n        os.path.join(args.save_path, 'relation_embedding'), \n        relation_embedding\n    )\n\ndef set_logger(args):\n    '''\n    Write logs to checkpoint and console\n    '''\n\n    if args.do_train:\n        log_file = os.path.join(args.save_path or args.init_checkpoint, 'train.log')\n    else:\n        log_file = os.path.join(args.save_path or args.init_checkpoint, 'test.log')\n\n    logging.basicConfig(\n        format='%(asctime)s %(levelname)-8s %(message)s',\n        level=logging.INFO,\n        datefmt='%Y-%m-%d %H:%M:%S',\n        filename=log_file,\n        filemode='w'\n    )\n\n    if args.print_on_screen:\n        console = logging.StreamHandler()\n        console.setLevel(logging.INFO)\n        formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n        console.setFormatter(formatter)\n        logging.getLogger('').addHandler(console)\n\ndef log_metrics(mode, step, metrics, writer):\n    '''\n    Print the evaluation logs\n    '''\n    for metric in metrics:\n        logging.info('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n        writer.add_scalar(\"_\".join([mode, metric]), metrics[metric], step)\n        \n        \ndef main(args):\n    if (not args.do_train) and (not args.do_valid) and (not args.do_test) and (not args.evaluate_train):\n        raise ValueError('one of train/val/test mode must be choosed.')\n    \n    if args.init_checkpoint:\n        override_config(args)\n\n    args.save_path = 'log/%s/%s/%s-%s/%s'%(args.dataset, args.model, args.hidden_dim, args.gamma, time.time()) if args.save_path == None else args.save_path\n    writer = SummaryWriter(args.save_path)\n    \n    # Write logs to checkpoint and console\n    set_logger(args)\n    \n    dataset = LinkPropPredDataset(name = 'ogbl-biokg')\n    split_edge = dataset.get_edge_split()\n    train_triples, valid_triples, test_triples = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]\n    nrelation = int(max(train_triples['relation']))+1\n    entity_dict = dict()\n    cur_idx = 0\n    for key in dataset[0]['num_nodes_dict']:\n        entity_dict[key] = (cur_idx, cur_idx + dataset[0]['num_nodes_dict'][key])\n        cur_idx += dataset[0]['num_nodes_dict'][key]\n    nentity = sum(dataset[0]['num_nodes_dict'].values())\n\n    evaluator = Evaluator(name = args.dataset)\n\n    args.nentity = nentity\n    args.nrelation = nrelation\n    \n    logging.info('Model: %s' % args.model)\n    logging.info('Dataset: %s' % args.dataset)\n    logging.info('#entity: %d' % nentity)\n    logging.info('#relation: %d' % nrelation)\n    \n    # train_triples = split_dict['train']\n    logging.info('#train: %d' % len(train_triples['head']))\n    # valid_triples = split_dict['valid']\n    logging.info('#valid: %d' % len(valid_triples['head']))\n    # test_triples = split_dict['test']\n    logging.info('#test: %d' % len(test_triples['head']))\n\n    train_count, train_true_head, train_true_tail = defaultdict(lambda: 4), defaultdict(list), defaultdict(list)\n    for i in tqdm(range(len(train_triples['head']))):\n        head, relation, tail = train_triples['head'][i], train_triples['relation'][i], train_triples['tail'][i]\n        head_type, tail_type = train_triples['head_type'][i], train_triples['tail_type'][i]\n        train_count[(head, relation, head_type)] += 1\n        train_count[(tail, -relation-1, tail_type)] += 1\n        train_true_head[(relation, tail)].append(head)\n        train_true_tail[(head, relation)].append(tail)\n    \n    kge_model = KGEModel(\n        model_name=args.model,\n        nentity=nentity,\n        nrelation=nrelation,\n        hidden_dim=args.hidden_dim,\n        gamma=args.gamma,\n        double_entity_embedding=args.double_entity_embedding,\n        double_relation_embedding=args.double_relation_embedding,\n        evaluator=evaluator\n    )\n    \n    logging.info('Model Parameter Configuration:')\n    for name, param in kge_model.named_parameters():\n        logging.info('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n\n    if args.cuda:\n        kge_model = kge_model.cuda()\n    \n    if args.init_checkpoint:\n        # Restore model from checkpoint directory\n        logging.info('Loading checkpoint %s...' % args.init_checkpoint)\n        checkpoint = torch.load(os.path.join(args.init_checkpoint, 'checkpoint'))\n        entity_dict = checkpoint['entity_dict']\n\n    if args.do_train:\n        # Set training dataloader iterator\n        train_dataloader_head = DataLoader(\n            TrainDataset(train_triples, nentity, nrelation, \n                args.negative_sample_size, 'head-batch',\n                train_count, train_true_head, train_true_tail,\n                entity_dict), \n            batch_size=args.batch_size,\n            shuffle=True, \n            num_workers=max(1, args.cpu_num//2),\n            collate_fn=TrainDataset.collate_fn\n        )\n        \n        train_dataloader_tail = DataLoader(\n            TrainDataset(train_triples, nentity, nrelation, \n                args.negative_sample_size, 'tail-batch',\n                train_count, train_true_head, train_true_tail,\n                entity_dict), \n            batch_size=args.batch_size,\n            shuffle=True, \n            num_workers=max(1, args.cpu_num//2),\n            collate_fn=TrainDataset.collate_fn\n        )\n        \n        train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)\n        \n        # Set training configuration\n        current_learning_rate = args.learning_rate\n        optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, kge_model.parameters()), \n            lr=current_learning_rate\n        )\n        if args.warm_up_steps:\n            warm_up_steps = args.warm_up_steps\n        else:\n            warm_up_steps = args.max_steps // 2\n\n    if args.init_checkpoint:\n        # Restore model from checkpoint directory\n        # logging.info('Loading checkpoint %s...' % args.init_checkpoint)\n        # checkpoint = torch.load(os.path.join(args.init_checkpoint, 'checkpoint'))\n        init_step = checkpoint['step']\n        kge_model.load_state_dict(checkpoint['model_state_dict'])\n        # entity_dict = checkpoint['entity_dict']\n        if args.do_train:\n            current_learning_rate = checkpoint['current_learning_rate']\n            warm_up_steps = checkpoint['warm_up_steps']\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    else:\n        logging.info('Ramdomly Initializing %s Model...' % args.model)\n        init_step = 0\n    \n    step = init_step\n    \n    logging.info('Start Training...')\n    logging.info('init_step = %d' % init_step)\n    logging.info('batch_size = %d' % args.batch_size)\n    logging.info('negative_adversarial_sampling = %d' % args.negative_adversarial_sampling)\n    logging.info('hidden_dim = %d' % args.hidden_dim)\n    logging.info('gamma = %f' % args.gamma)\n    logging.info('negative_adversarial_sampling = %s' % str(args.negative_adversarial_sampling))\n    if args.negative_adversarial_sampling:\n        logging.info('adversarial_temperature = %f' % args.adversarial_temperature)\n    \n    # Set valid dataloader as it would be evaluated during training\n    \n    if args.do_train:\n        logging.info('learning_rate = %d' % current_learning_rate)\n\n        training_logs = []\n        \n        #Training Loop\n        for step in range(init_step, args.max_steps):\n            \n            log = kge_model.train_step(kge_model, optimizer, train_iterator, args)\n            training_logs.append(log)\n            \n            if step >= warm_up_steps:\n                current_learning_rate = current_learning_rate / 10\n                logging.info('Change learning_rate to %f at step %d' % (current_learning_rate, step))\n                optimizer = torch.optim.Adam(\n                    filter(lambda p: p.requires_grad, kge_model.parameters()), \n                    lr=current_learning_rate\n                )\n                warm_up_steps = warm_up_steps * 3\n            \n            if step % args.save_checkpoint_steps == 0 and step > 0: # ~ 41 seconds/saving\n                save_variable_list = {\n                    'step': step, \n                    'current_learning_rate': current_learning_rate,\n                    'warm_up_steps': warm_up_steps,\n                    'entity_dict': entity_dict\n                }\n                save_model(kge_model, optimizer, save_variable_list, args)\n\n            if step % args.log_steps == 0:\n                metrics = {}\n                for metric in training_logs[0].keys():\n                    metrics[metric] = sum([log[metric] for log in training_logs])/len(training_logs)\n                log_metrics('Train', step, metrics, writer)\n                training_logs = []\n                \n            if args.do_valid and step % args.valid_steps == 0 and step > 0:\n                logging.info('Evaluating on Valid Dataset...')\n                metrics = kge_model.test_step(kge_model, valid_triples, args, entity_dict)\n                log_metrics('Valid', step, metrics, writer)\n        \n        save_variable_list = {\n            'step': step, \n            'current_learning_rate': current_learning_rate,\n            'warm_up_steps': warm_up_steps\n        }\n        save_model(kge_model, optimizer, save_variable_list, args)\n        \n    if args.do_valid:\n        logging.info('Evaluating on Valid Dataset...')\n        metrics = kge_model.test_step(kge_model, valid_triples, args, entity_dict)\n        log_metrics('Valid', step, metrics, writer)\n    \n    if args.do_test:\n        logging.info('Evaluating on Test Dataset...')\n        metrics = kge_model.test_step(kge_model, test_triples, args, entity_dict)\n        log_metrics('Test', step, metrics, writer)\n    \n    if args.evaluate_train:\n        logging.info('Evaluating on Training Dataset...')\n        small_train_triples = {}\n        indices = np.random.choice(len(train_triples['head']), args.ntriples_eval_train, replace=False)\n        for i in train_triples:\n            if 'type' in i:\n                small_train_triples[i] = [train_triples[i][x] for x in indices]\n            else:\n                small_train_triples[i] = train_triples[i][indices]\n        metrics = kge_model.test_step(kge_model, small_train_triples, args, entity_dict, random_sampling=True)\n        log_metrics('Train', step, metrics, writer)\n        \nif __name__ == '__main__':\n    main(parse_args())\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/examples/linkproppred/biokg/run.py b/examples/linkproppred/biokg/run.py
--- a/examples/linkproppred/biokg/run.py	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/examples/linkproppred/biokg/run.py	(date 1649338105452)
@@ -35,7 +35,7 @@
 
     parser.add_argument('--cuda', action='store_true', help='use GPU')
     
-    parser.add_argument('--do_train', action='store_true')
+    parser.add_argument('--do_train', action='store_false')
     parser.add_argument('--do_valid', action='store_true')
     parser.add_argument('--do_test', action='store_true')
     parser.add_argument('--evaluate_train', action='store_true', help='Evaluate on training data')
Index: ogb/nodeproppred/.~lock.master.csv#
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ogb/nodeproppred/.~lock.master.csv# b/ogb/nodeproppred/.~lock.master.csv#
new file mode 100755
--- /dev/null	(date 1649179538069)
+++ b/ogb/nodeproppred/.~lock.master.csv#	(date 1649179538069)
@@ -0,0 +1,1 @@
+,hussein,hussein-XPS-15-9500,05.04.2022 13:25,file:///home/hussein/.config/libreoffice/4;
\ No newline at end of file
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
new file mode 100755
--- /dev/null	(date 1649617000351)
+++ b/.idea/workspace.xml	(date 1649617000351)
@@ -0,0 +1,322 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="AutoImportSettings">
+    <option name="autoReloadType" value="SELECTIVE" />
+  </component>
+  <component name="ChangeListManager">
+    <list default="true" id="d7625012-b420-4433-8a77-9af082890cd1" name="Changes" comment="OGB_MAG_QM">
+      <change afterPath="$PROJECT_DIR$/.idea/csv-plugin.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/inspectionProfiles/profiles_settings.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/misc.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/modules.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/ogb.iml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/vcs.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/TSV_TO_Hetro_OGB.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/examples/nodeproppred/mag/ogbn-mag-FM-GCN.model" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/examples/nodeproppred/mag/ogbn-mag-FM-GSaint.model" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/examples/nodeproppred/mag/ogbn-mag-FM-RGCN.model" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/ogb/nodeproppred/.~lock.master.csv#" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.gitignore" beforeDir="false" afterPath="$PROJECT_DIR$/.gitignore" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/examples/linkproppred/biokg/run.py" beforeDir="false" afterPath="$PROJECT_DIR$/examples/linkproppred/biokg/run.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/examples/nodeproppred/mag/cluster_gcn.py" beforeDir="false" afterPath="$PROJECT_DIR$/examples/nodeproppred/mag/cluster_gcn.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/examples/nodeproppred/mag/gnn.py" beforeDir="false" afterPath="$PROJECT_DIR$/examples/nodeproppred/mag/gnn.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/examples/nodeproppred/mag/graph_saint.py" beforeDir="false" afterPath="$PROJECT_DIR$/examples/nodeproppred/mag/graph_saint.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/examples/nodeproppred/mag/rgcn.py" beforeDir="false" afterPath="$PROJECT_DIR$/examples/nodeproppred/mag/rgcn.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/examples/nodeproppred/products/gnn.py" beforeDir="false" afterPath="$PROJECT_DIR$/examples/nodeproppred/products/gnn.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/ogb/nodeproppred/dataset_pyg.py" beforeDir="false" afterPath="$PROJECT_DIR$/ogb/nodeproppred/dataset_pyg.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/ogb/nodeproppred/master.csv" beforeDir="false" afterPath="$PROJECT_DIR$/ogb/nodeproppred/master.csv" afterDir="false" />
+    </list>
+    <option name="SHOW_DIALOG" value="false" />
+    <option name="HIGHLIGHT_CONFLICTS" value="true" />
+    <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
+    <option name="LAST_RESOLUTION" value="IGNORE" />
+  </component>
+  <component name="FileTemplateManagerImpl">
+    <option name="RECENT_TEMPLATES">
+      <list>
+        <option value="Python Script" />
+      </list>
+    </option>
+  </component>
+  <component name="Git.Settings">
+    <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
+  </component>
+  <component name="GitSEFilterConfiguration">
+    <file-type-list>
+      <filtered-out-file-type name="LOCAL_BRANCH" />
+      <filtered-out-file-type name="REMOTE_BRANCH" />
+      <filtered-out-file-type name="TAG" />
+      <filtered-out-file-type name="COMMIT_BY_MESSAGE" />
+    </file-type-list>
+  </component>
+  <component name="MarkdownSettingsMigration">
+    <option name="stateVersion" value="1" />
+  </component>
+  <component name="ProjectId" id="26tjtOrkcXrJJxPbLVmzT1VSYHU" />
+  <component name="ProjectLevelVcsManager" settingsEditedManually="true" />
+  <component name="ProjectViewState">
+    <option name="hideEmptyMiddlePackages" value="true" />
+    <option name="showLibraryContents" value="true" />
+  </component>
+  <component name="PropertiesComponent">
+    <property name="ASKED_SHARE_PROJECT_CONFIGURATION_FILES" value="true" />
+    <property name="RunOnceActivity.OpenProjectViewOnStart" value="true" />
+    <property name="RunOnceActivity.ShowReadmeOnStart" value="true" />
+    <property name="SHARE_PROJECT_CONFIGURATION_FILES" value="true" />
+    <property name="last_opened_file_path" value="$PROJECT_DIR$" />
+    <property name="run.code.analysis.last.selected.profile" value="aDefault" />
+    <property name="settings.editor.selected.configurable" value="project.scopes" />
+  </component>
+  <component name="RunManager" selected="Python.TSV_TO_Hetro_OGB">
+    <configuration name="TSV_TO_Hetro_OGB" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="ogb" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/TSV_TO_Hetro_OGB.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+    <configuration name="gnn" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="ogb" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/examples/nodeproppred/mag" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/examples/nodeproppred/mag/gnn.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+    <configuration name="graph_saint" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="ogb" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/examples/nodeproppred/mag" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/examples/nodeproppred/mag/graph_saint.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+    <configuration name="rgcn" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="ogb" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/examples/nodeproppred/mag" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/examples/nodeproppred/mag/rgcn.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+    <configuration name="run" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="ogb" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/examples/linkproppred/biokg" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/examples/linkproppred/biokg/run.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+    <recent_temporary>
+      <list>
+        <item itemvalue="Python.TSV_TO_Hetro_OGB" />
+        <item itemvalue="Python.graph_saint" />
+        <item itemvalue="Python.gnn" />
+        <item itemvalue="Python.rgcn" />
+        <item itemvalue="Python.run" />
+      </list>
+    </recent_temporary>
+  </component>
+  <component name="SpellCheckerSettings" RuntimeDictionaries="0" Folders="0" CustomDictionaries="0" DefaultDictionary="application-level" UseSingleDictionary="true" transferred="true" />
+  <component name="TaskManager">
+    <task active="true" id="Default" summary="Default task">
+      <changelist id="d7625012-b420-4433-8a77-9af082890cd1" name="Changes" comment="" />
+      <created>1648249990896</created>
+      <option name="number" value="Default" />
+      <option name="presentableId" value="Default" />
+      <updated>1648249990896</updated>
+    </task>
+    <task id="LOCAL-00001" summary="OGB_MAG_QM">
+      <created>1649608059075</created>
+      <option name="number" value="00001" />
+      <option name="presentableId" value="LOCAL-00001" />
+      <option name="project" value="LOCAL" />
+      <updated>1649608059075</updated>
+    </task>
+    <task id="LOCAL-00002" summary="OGB_MAG_QM">
+      <created>1649609374468</created>
+      <option name="number" value="00002" />
+      <option name="presentableId" value="LOCAL-00002" />
+      <option name="project" value="LOCAL" />
+      <updated>1649609374468</updated>
+    </task>
+    <task id="LOCAL-00003" summary="OGB_MAG_QM">
+      <created>1649616400175</created>
+      <option name="number" value="00003" />
+      <option name="presentableId" value="LOCAL-00003" />
+      <option name="project" value="LOCAL" />
+      <updated>1649616400175</updated>
+    </task>
+    <option name="localTasksCounter" value="4" />
+    <servers />
+  </component>
+  <component name="Vcs.Log.Tabs.Properties">
+    <option name="TAB_STATES">
+      <map>
+        <entry key="MAIN">
+          <value>
+            <State />
+          </value>
+        </entry>
+      </map>
+    </option>
+  </component>
+  <component name="VcsManagerConfiguration">
+    <MESSAGE value="OGB_MAG_QM" />
+    <option name="LAST_COMMIT_MESSAGE" value="OGB_MAG_QM" />
+  </component>
+  <component name="XDebuggerManager">
+    <breakpoint-manager>
+      <breakpoints>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/examples/nodeproppred/mag/rgcn.py</url>
+          <line>93</line>
+          <option name="timeStamp" value="7" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/TSV_TO_Hetro_OGB.py</url>
+          <line>234</line>
+          <option name="timeStamp" value="11" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/TSV_TO_Hetro_OGB.py</url>
+          <line>229</line>
+          <option name="timeStamp" value="14" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/ogb/io/read_graph_raw.py</url>
+          <line>32</line>
+          <option name="timeStamp" value="40" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/ogb/graphproppred/dataset.py</url>
+          <line>111</line>
+          <option name="timeStamp" value="41" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/ogb/graphproppred/dataset.py</url>
+          <line>61</line>
+          <option name="timeStamp" value="42" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/ogb/io/read_graph_pyg.py</url>
+          <line>57</line>
+          <option name="timeStamp" value="43" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/ogb/nodeproppred/dataset_pyg.py</url>
+          <line>131</line>
+          <option name="timeStamp" value="44" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/ogb/nodeproppred/dataset_pyg.py</url>
+          <line>55</line>
+          <option name="timeStamp" value="46" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/ogb/nodeproppred/dataset_pyg.py</url>
+          <line>179</line>
+          <option name="timeStamp" value="47" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/TSV_TO_Hetro_OGB.py</url>
+          <line>167</line>
+          <option name="timeStamp" value="48" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/TSV_TO_Hetro_OGB.py</url>
+          <line>20</line>
+          <option name="timeStamp" value="52" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/examples/nodeproppred/mag/rgcn.py</url>
+          <line>229</line>
+          <option name="timeStamp" value="53" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/examples/nodeproppred/mag/graph_saint.py</url>
+          <line>332</line>
+          <option name="timeStamp" value="56" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/TSV_TO_Hetro_OGB.py</url>
+          <line>14</line>
+          <option name="timeStamp" value="57" />
+        </line-breakpoint>
+      </breakpoints>
+    </breakpoint-manager>
+    <watches-manager>
+      <configuration name="PythonConfigurationType">
+        <watch expression="out[train_idx]" language="Python" />
+        <watch expression="y_true[train_idx]" language="Python" />
+      </configuration>
+    </watches-manager>
+  </component>
+</project>
\ No newline at end of file
Index: ogb/nodeproppred/master.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>,ogbn-proteins,ogbn-products,ogbn-arxiv,ogbn-mag,ogbn-papers100M\nnum tasks,112,1,1,1,1\nnum classes,2,47,40,349,172\neval metric,rocauc,acc,acc,acc,acc\ntask type,binary classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification\ndownload_name,proteins,products,arxiv,mag,papers100M-bin\nversion,1,1,1,2,1\nurl,http://snap.stanford.edu/ogb/data/nodeproppred/proteins.zip,http://snap.stanford.edu/ogb/data/nodeproppred/products.zip,http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip,http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip,http://snap.stanford.edu/ogb/data/nodeproppred/papers100M-bin.zip\nadd_inverse_edge,True,True,False,False,False\nhas_node_attr,False,True,True,True,True\nhas_edge_attr,True,False,False,False,False\nsplit,species,sales_ranking,time,time,time\nadditional node files,node_species,None,node_year,node_year,node_year\nadditional edge files,None,None,None,edge_reltype,None\nis hetero,False,False,False,True,False\nbinary,False,False,False,False,True\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ogb/nodeproppred/master.csv b/ogb/nodeproppred/master.csv
--- a/ogb/nodeproppred/master.csv	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/ogb/nodeproppred/master.csv	(date 1649219109573)
@@ -1,16 +1,16 @@
-,ogbn-proteins,ogbn-products,ogbn-arxiv,ogbn-mag,ogbn-papers100M
-num tasks,112,1,1,1,1
-num classes,2,47,40,349,172
-eval metric,rocauc,acc,acc,acc,acc
-task type,binary classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification
-download_name,proteins,products,arxiv,mag,papers100M-bin
-version,1,1,1,2,1
-url,http://snap.stanford.edu/ogb/data/nodeproppred/proteins.zip,http://snap.stanford.edu/ogb/data/nodeproppred/products.zip,http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip,http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip,http://snap.stanford.edu/ogb/data/nodeproppred/papers100M-bin.zip
-add_inverse_edge,True,True,False,False,False
-has_node_attr,False,True,True,True,True
-has_edge_attr,True,False,False,False,False
-split,species,sales_ranking,time,time,time
-additional node files,node_species,None,node_year,node_year,node_year
-additional edge files,None,None,None,edge_reltype,None
-is hetero,False,False,False,True,False
-binary,False,False,False,False,True
+,ogbn-proteins,ogbn-products,ogbn-arxiv,ogbn-mag,ogbn-papers100M,ogbn-mag-QM1,ogbn-mag-QM2,ogbn-mag-QM3,ogbn-mag-QM4,ogbn-mag-QM0,ogbn-mag-QM6,ogbn-mag-QM7,ogbn-mag-QM8,ogbn-mag-QM9,ogbn-mag-QM10
+num tasks,112,1,1,1,1,1,1,1,1,1,1,1,1,1,1
+num classes,2,47,40,349,172,175,349,258,246,349,349,349,349,349,349
+eval metric,rocauc,acc,acc,acc,acc,acc,acc,acc,acc,acc,acc,acc,acc,acc,acc
+task type,binary classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification,multiclass classification
+download_name,proteins,products,arxiv,mag,papers100M-bin,mag_AtmosphericSciences_papers_venue_QM1,mag_Diffraction_papers_venue_QM2,mag_ComputerProgramming_papers_venue_QM3,mag_QuantumElectrodynamics_papers_venue_QM4,OBGN-MAG-QM0,mag,mag,mag,mag,mag
+version,1,1,1,2,1,2,2,2,2,2,2,2,2,2,2
+url,http://snap.stanford.edu/ogb/data/nodeproppred/proteins.zip,/media/hussein/UbuntuData/GithubRepos/ogb/Amazon_products.zip,http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip,http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip,http://snap.stanford.edu/ogb/data/nodeproppred/papers100M-bin.zip,/media/hussein/UbuntuData/GithubRepos/ogb/mag_AtmosphericSciences_papers_venue_QM1.zip,/media/hussein/UbuntuData/GithubRepos/ogb/mag_Diffraction_papers_venue_QM2.zip,/media/hussein/UbuntuData/GithubRepos/ogb/mag_ComputerProgramming_papers_venue_QM3.zip,/media/hussein/UbuntuData/GithubRepos/ogb/mag_QuantumElectrodynamics_papers_venue_QM4.zip,/media/hussein/UbuntuData/GithubRepos/ogb/OBGN-MAG-QM0.zip,http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip,http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip,http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip,http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip,http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip
+add_inverse_edge,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False
+has_node_attr,False,True,True,True,True,True,True,True,True,True,True,True,True,True,True
+has_edge_attr,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False
+split,species,sales_ranking,time,time,time,time,time,time,time,time,time,time,time,time,time
+additional node files,node_species,None,node_year,node_year,node_year,node_year,node_year,node_year,node_year,node_year,node_year,node_year,node_year,node_year,node_year
+additional edge files,None,None,None,edge_reltype,None,edge_reltype,edge_reltype,edge_reltype,edge_reltype,edge_reltype,edge_reltype,edge_reltype,edge_reltype,edge_reltype,edge_reltype
+is hetero,False,False,False,True,False,True,True,True,True,True,True,True,True,True,True
+binary,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False
Index: ogb/nodeproppred/dataset_pyg.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from torch_geometric.data import InMemoryDataset\nimport pandas as pd\nimport shutil, os\nimport os.path as osp\nimport torch\nimport numpy as np\nfrom ogb.utils.url import decide_download, download_url, extract_zip\nfrom ogb.io.read_graph_pyg import read_graph_pyg, read_heterograph_pyg\nfrom ogb.io.read_graph_raw import read_node_label_hetero, read_nodesplitidx_split_hetero\n\nclass PygNodePropPredDataset(InMemoryDataset):\n    def __init__(self, name, root = 'dataset', transform=None, pre_transform=None, meta_dict = None):\n        '''\n            - name (str): name of the dataset\n            - root (str): root directory to store the dataset folder\n            - transform, pre_transform (optional): transform/pre-transform graph objects\n\n            - meta_dict: dictionary that stores all the meta-information about data. Default is None, \n                    but when something is passed, it uses its information. Useful for debugging for external contributers.\n        ''' \n\n        self.name = name ## original name, e.g., ogbn-proteins\n\n        if meta_dict is None:\n            self.dir_name = '_'.join(name.split('-')) \n            \n            # check if previously-downloaded folder exists.\n            # If so, use that one.\n            if osp.exists(osp.join(root, self.dir_name + '_pyg')):\n                self.dir_name = self.dir_name + '_pyg'\n\n            self.original_root = root\n            self.root = osp.join(root, self.dir_name)\n            \n            master = pd.read_csv(os.path.join(os.path.dirname(__file__), 'master.csv'), index_col = 0)\n            if not self.name in master:\n                error_mssg = 'Invalid dataset name {}.\\n'.format(self.name)\n                error_mssg += 'Available datasets are as follows:\\n'\n                error_mssg += '\\n'.join(master.keys())\n                raise ValueError(error_mssg)\n            self.meta_info = master[self.name]\n            \n        else:\n            self.dir_name = meta_dict['dir_path']\n            self.original_root = ''\n            self.root = meta_dict['dir_path']\n            self.meta_info = meta_dict\n\n        # check version\n        # First check whether the dataset has been already downloaded or not.\n        # If so, check whether the dataset version is the newest or not.\n        # If the dataset is not the newest version, notify this to the user. \n        if osp.isdir(self.root) and (not osp.exists(osp.join(self.root, 'RELEASE_v' + str(self.meta_info['version']) + '.txt'))):\n            print(self.name + ' has been updated.')\n            if input('Will you update the dataset now? (y/N)\\n').lower() == 'y':\n                shutil.rmtree(self.root)\n\n\n        self.download_name = self.meta_info['download_name'] ## name of downloaded file, e.g., tox21\n\n        self.num_tasks = int(self.meta_info['num tasks'])\n        self.task_type = self.meta_info['task type']\n        self.eval_metric = self.meta_info['eval metric']\n        self.__num_classes__ = int(self.meta_info['num classes'])\n        self.is_hetero = self.meta_info['is hetero'] == 'True'\n        self.binary = self.meta_info['binary'] == 'True'\n\n        super(PygNodePropPredDataset, self).__init__(self.root, transform, pre_transform)\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    def get_idx_split(self, split_type = None):\n        if split_type is None:\n            split_type = self.meta_info['split']\n\n        path = osp.join(self.root, 'split', split_type)\n\n        # short-cut if split_dict.pt exists\n        if os.path.isfile(os.path.join(path, 'split_dict.pt')):\n            return torch.load(os.path.join(path, 'split_dict.pt'))\n\n        if self.is_hetero:\n            train_idx_dict, valid_idx_dict, test_idx_dict = read_nodesplitidx_split_hetero(path)\n            for nodetype in train_idx_dict.keys():\n                train_idx_dict[nodetype] = torch.from_numpy(train_idx_dict[nodetype]).to(torch.long)\n                valid_idx_dict[nodetype] = torch.from_numpy(valid_idx_dict[nodetype]).to(torch.long)\n                test_idx_dict[nodetype] = torch.from_numpy(test_idx_dict[nodetype]).to(torch.long)\n\n                return {'train': train_idx_dict, 'valid': valid_idx_dict, 'test': test_idx_dict}\n\n        else:\n            train_idx = torch.from_numpy(pd.read_csv(osp.join(path, 'train.csv.gz'), compression='gzip', header = None).values.T[0]).to(torch.long)\n            valid_idx = torch.from_numpy(pd.read_csv(osp.join(path, 'valid.csv.gz'), compression='gzip', header = None).values.T[0]).to(torch.long)\n            test_idx = torch.from_numpy(pd.read_csv(osp.join(path, 'test.csv.gz'), compression='gzip', header = None).values.T[0]).to(torch.long)\n\n            return {'train': train_idx, 'valid': valid_idx, 'test': test_idx}\n\n    @property\n    def num_classes(self):\n        return self.__num_classes__\n\n    @property\n    def raw_file_names(self):\n        if self.binary:\n            if self.is_hetero:\n                return ['edge_index_dict.npz']\n            else:\n                return ['data.npz']\n        else:\n            if self.is_hetero:\n                return ['num-node-dict.csv.gz', 'triplet-type-list.csv.gz']\n            else:\n                file_names = ['edge']\n                if self.meta_info['has_node_attr'] == 'True':\n                    file_names.append('node-feat')\n                if self.meta_info['has_edge_attr'] == 'True':\n                    file_names.append('edge-feat')\n                return [file_name + '.csv.gz' for file_name in file_names]\n\n    @property\n    def processed_file_names(self):\n        return osp.join('geometric_data_processed.pt')\n\n    def download(self):\n        url =  self.meta_info['url']\n        if decide_download(url):\n            path = download_url(url, self.original_root)\n            extract_zip(path, self.original_root)\n            os.unlink(path)\n            shutil.rmtree(self.root)\n            shutil.move(osp.join(self.original_root, self.download_name), self.root)\n        else:\n            print('Stop downloading.')\n            shutil.rmtree(self.root)\n            exit(-1)\n\n    def process(self):\n        add_inverse_edge = self.meta_info['add_inverse_edge'] == 'True'\n\n        if self.meta_info['additional node files'] == 'None':\n            additional_node_files = []\n        else:\n            additional_node_files = self.meta_info['additional node files'].split(',')\n\n        if self.meta_info['additional edge files'] == 'None':\n            additional_edge_files = []\n        else:\n            additional_edge_files = self.meta_info['additional edge files'].split(',')\n\n        if self.is_hetero:\n            data = read_heterograph_pyg(self.raw_dir, add_inverse_edge = add_inverse_edge, additional_node_files = additional_node_files, additional_edge_files = additional_edge_files, binary=self.binary)[0]\n\n            if self.binary:\n                tmp = np.load(osp.join(self.raw_dir, 'node-label.npz'))\n                node_label_dict = {}\n                for key in list(tmp.keys()):\n                    node_label_dict[key] = tmp[key]\n                del tmp\n            else:\n                node_label_dict = read_node_label_hetero(self.raw_dir)\n\n            data.y_dict = {}\n            if 'classification' in self.task_type:\n                for nodetype, node_label in node_label_dict.items():\n                    # detect if there is any nan\n                    if np.isnan(node_label).any():\n                        data.y_dict[nodetype] = torch.from_numpy(node_label).to(torch.float32)\n                    else:\n                        data.y_dict[nodetype] = torch.from_numpy(node_label).to(torch.long)\n            else:\n                for nodetype, node_label in node_label_dict.items():\n                    data.y_dict[nodetype] = torch.from_numpy(node_label).to(torch.float32)\n\n        else:\n            data = read_graph_pyg(self.raw_dir, add_inverse_edge = add_inverse_edge, additional_node_files = additional_node_files, additional_edge_files = additional_edge_files, binary=self.binary)[0]\n\n            ### adding prediction target\n            if self.binary:\n                node_label = np.load(osp.join(self.raw_dir, 'node-label.npz'))['node_label']\n            else:\n                node_label = pd.read_csv(osp.join(self.raw_dir, 'node-label.csv.gz'), compression='gzip', header = None).values\n\n            if 'classification' in self.task_type:\n                # detect if there is any nan\n                if np.isnan(node_label).any():\n                    data.y = torch.from_numpy(node_label).to(torch.float32)\n                else:\n                    data.y = torch.from_numpy(node_label).to(torch.long)\n\n            else:\n                data.y = torch.from_numpy(node_label).to(torch.float32)\n\n        data = data if self.pre_transform is None else self.pre_transform(data)\n\n        print('Saving...')\n        torch.save(self.collate([data]), self.processed_paths[0])\n\n    def __repr__(self):\n        return '{}()'.format(self.__class__.__name__)\n        \n\nif __name__ == '__main__':\n    pyg_dataset = PygNodePropPredDataset(name = 'ogbn-mag')\n    print(pyg_dataset[0])\n    split_index = pyg_dataset.get_idx_split()\n    # print(split_index)\n    
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ogb/nodeproppred/dataset_pyg.py b/ogb/nodeproppred/dataset_pyg.py
--- a/ogb/nodeproppred/dataset_pyg.py	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/ogb/nodeproppred/dataset_pyg.py	(date 1649181302418)
@@ -122,7 +122,13 @@
 
     def download(self):
         url =  self.meta_info['url']
-        if decide_download(url):
+        if str(url).startswith("http")==False:
+            path =url
+            extract_zip(path, self.original_root)
+            os.unlink(path)
+            shutil.rmtree(self.root)
+            shutil.move(osp.join(self.original_root, self.download_name), self.root)
+        elif decide_download(url):
             path = download_url(url, self.original_root)
             extract_zip(path, self.original_root)
             os.unlink(path)
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>__pycache__/\nbuild/\ndist/\n*.egg-info/\ndataset/\n*.swp\n*.vscode\n*.DS_Store\n*.pt\n*.so\n*trial*\n*.ipynb\n*.sh\n*analyze*\n*random.py\n*RELEASE_*\n*.csv.gz\n*.zip\n*submission_\n*.npz\n*.npy\n**/convert.py\n**/mapping/README.md\n**.json\n**/checkpoint\n**/events.out.*\n**/test*\n**/pcqm4m-scaf/**\n**/logs/**\n**/lightning_logs/**
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.gitignore b/.gitignore
--- a/.gitignore	(revision 1e0c3dcdece6c96b9851bce973f0585bbd76a80e)
+++ b/.gitignore	(date 1649616299907)
@@ -8,6 +8,8 @@
 *.DS_Store
 *.pt
 *.so
+*.csv
+*.model
 *trial*
 *.ipynb
 *.sh
@@ -27,4 +29,5 @@
 **/test*
 **/pcqm4m-scaf/**
 **/logs/**
-**/lightning_logs/**
\ No newline at end of file
+**/lightning_logs/**
+/examples/linkproppred/biokg/log/ogbl-biokg/TransE/500-12.0/1649338106.4477725/train.log
Index: .idea/modules.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/modules.xml b/.idea/modules.xml
new file mode 100755
--- /dev/null	(date 1648250062157)
+++ b/.idea/modules.xml	(date 1648250062157)
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectModuleManager">
+    <modules>
+      <module fileurl="file://$PROJECT_DIR$/.idea/ogb.iml" filepath="$PROJECT_DIR$/.idea/ogb.iml" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file
Index: .idea/csv-plugin.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/csv-plugin.xml b/.idea/csv-plugin.xml
new file mode 100755
--- /dev/null	(date 1649616317150)
+++ b/.idea/csv-plugin.xml	(date 1649616317150)
@@ -0,0 +1,219 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="CsvFileAttributes">
+    <option name="attributeMap">
+      <map>
+        <entry key="/OBGN-MAG-QM0/mapping/author_entidx2name.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/OBGN-MAG-QM0/mapping/labelidx2labelname.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/OBGN-MAG-QM0/mapping/paper_entidx2name.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/OBGN-MAG-QM0/raw/node-label/paper/node-label.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/OBGN-MAG-QM0/split/time/paper/train.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/examples/nodeproppred/mag/GCN_mag_output.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/examples/nodeproppred/mag/GSaint_mag_output.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/examples/nodeproppred/mag/RGCN_mag_output.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/mapping/author_entidx2name.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/mapping/paper_entidx2name.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/mapping/relidx2relname.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/raw/node-label/paper/node-label.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/raw/relations/paper___cites___paper/edge.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/raw/relations/paper___has_topic___field_of_study/edge.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/split/time/paper/test.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/split/time/paper/train.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_AtmosphericSciences_papers_venue_QM1/split/time/paper/valid.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_ComputerProgramming_papers_venue_QM3/mapping/paper_entidx2name.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_ComputerProgramming_papers_venue_QM3/raw/relations/paper___writtenby___author/edge.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_ComputerProgramming_papers_venue_QM3/split/time/paper/train.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_ComputerProgramming_papers_venue_QM3/split/time/paper/valid.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_Diffraction_papers_venue_QM2/mapping/paper_entidx2name.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_Diffraction_papers_venue_QM2/split/time/paper/train.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_Diffraction_papers_venue_QM2/split/time/paper/valid.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_QuantumElectrodynamics_papers_venue_QM4/mapping/author_entidx2name.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_QuantumElectrodynamics_papers_venue_QM4/raw/node-label/paper/node-label.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_QuantumElectrodynamics_papers_venue_QM4/raw/triplet-type-list.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/mag_QuantumElectrodynamics_papers_venue_QM4/split/time/paper/train.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/ogb/nodeproppred/master.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+        <entry key="/relidx2relname.csv">
+          <value>
+            <Attribute>
+              <option name="separator" value="," />
+            </Attribute>
+          </value>
+        </entry>
+      </map>
+    </option>
+  </component>
+</project>
\ No newline at end of file
